{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37a3115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri011228/ai2-server/deeplearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:55:28 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 07:55:31,375\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:55:40 [config.py:689] This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-12 07:55:40 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-12 07:55:41 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/home/yuri011228/ai2-server/models/mistral-7b', speculative_config=None, tokenizer='/home/yuri011228/ai2-server/models/mistral-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/yuri011228/ai2-server/models/mistral-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-12 07:55:42 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f922bf83fd0>\n",
      "INFO 05-12 07:55:43 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 07:55:43 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-12 07:55:43 [gpu_model_runner.py:1276] Starting to load model /home/yuri011228/ai2-server/models/mistral-7b...\n",
      "WARNING 05-12 07:55:44 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:24<00:48, 24.24s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:46<00:22, 22.98s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:10<00:00, 23.47s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:10<00:00, 23.46s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:56:54 [loader.py:458] Loading weights took 70.49 seconds\n",
      "INFO 05-12 07:56:54 [gpu_model_runner.py:1291] Model loading took 13.4967 GiB and 70.787281 seconds\n",
      "INFO 05-12 07:57:08 [backends.py:416] Using cache directory: /home/yuri011228/.cache/vllm/torch_compile_cache/8f3f217165/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-12 07:57:08 [backends.py:426] Dynamo bytecode transform time: 13.15 s\n",
      "INFO 05-12 07:57:09 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 05-12 07:57:17 [monitor.py:33] torch.compile takes 13.15 s in total\n",
      "INFO 05-12 07:57:19 [kv_cache_utils.py:634] GPU KV cache size: 49,632 tokens\n",
      "INFO 05-12 07:57:19 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 12.12x\n",
      "INFO 05-12 07:57:50 [gpu_model_runner.py:1626] Graph capturing finished in 31 secs, took 0.51 GiB\n",
      "INFO 05-12 07:57:50 [core.py:163] init engine (profile, create kv cache, warmup model) took 55.79 seconds\n",
      "INFO 05-12 07:57:50 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "mistral = \"/home/yuri011228/ai2-server/models/mistral-7b\"\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=mistral,\n",
    "    tensor_parallel_size=1, # GPU ê°œìˆ˜\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_num_seqs = 100, # ë™ì‹œì— ë°›ì„ ìˆ˜ ìˆëŠ” ìš”ì²­ ê°œìˆ˜\n",
    "    max_model_len=4096, # input + output í† í° ê¸¸ì´\n",
    "    max_num_batched_tokens=8192) \n",
    "\n",
    "llm = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# í™•ì¸ìš©\n",
    "# os.environ[\"LANGSMITH_TRACING\"]  \n",
    "# print(os.getenv(\"QDRANT_HOST\"))\n",
    "# print(os.getenv(\"QDRANT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f29ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = {\n",
    "  \"email\": \"ConconDev\",\n",
    "  \"date\": \"2024-09-06\",\n",
    "  \"level\": 1,\n",
    "  \"title\": \"ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ ê°œì„ : SseEmitter í™œìš© ë° ì‚¬ìš©ìë³„ Emitter ê´€ë¦¬\",\n",
    "  \"keywords\": [\n",
    "    \"SSE\",\n",
    "    \"SseEmitter\",\n",
    "    \"ì•ŒëŒ êµ¬ë…\",\n",
    "    \"ì‚¬ìš©ìë³„ ê´€ë¦¬\"\n",
    "  ],\n",
    "  \"til\": \"# ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ ê°œì„ : SseEmitter í™œìš© ë° ì‚¬ìš©ìë³„ Emitter ê´€ë¦¬\\n\\n## 1. ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©\\n\\nì˜¤ëŠ˜ ì €ëŠ” ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ë¥¼ í™œìš©í•˜ê³ , ê° ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.  `SseEmitter`ëŠ” ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë° ìœ ìš©í•œ APIì…ë‹ˆë‹¤.\\n\\n## 2. ê°œë… ì •ë¦¬\\n\\n*   **SSE (Server-Sent Events):** ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë‹¨ë°©í–¥ í†µì‹ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì›¹ ê¸°ìˆ ì…ë‹ˆë‹¤. ì„œë²„ê°€ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë°œìƒ ì‹œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.\\n*   **SseEmitter:** SSE í†µì‹ ì„ ìœ„í•œ ê°ì²´ì…ë‹ˆë‹¤.  ë°ì´í„°ë¥¼ ë°œí–‰í•˜ê±°ë‚˜, ì—°ê²°ì„ ì¢…ë£Œí•˜ê±°ë‚˜, ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\\n*   **ConcurrentHashMap:** ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ì ‘ê·¼í•´ë„ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤.  ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ í•˜ê³  `SseEmitter`ë¥¼ ê°’ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\n## 3. í•´ë‹¹ ê°œë…ì´ í•„ìš”í•œ ì´ìœ \\n\\nê¸°ì¡´ ì•ŒëŒ êµ¬ë… ë°©ì‹ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ë°©ì‹ìœ¼ë¡œ, ì„œë²„ ë¶€í•˜ê°€ ì‹¬í•˜ê³  íš¨ìœ¨ì„±ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. `SseEmitter`ë¥¼ ì‚¬ìš©í•˜ë©´ ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí–ˆì„ ë•Œë§Œ í´ë¼ì´ì–¸íŠ¸ì— ë°ì´í„°ë¥¼ ì „ì†¡í•˜ë¯€ë¡œ, ì„œë²„ ìì›ì„ ì ˆì•½í•˜ê³  ì‹¤ì‹œê°„ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨, íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n## 4. ê°œë…ì„ í™œìš©í•˜ëŠ” ë°©ë²•\\n\\n1.  `SseEmitter` ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\\n2.  ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ, `SseEmitter` ê°ì²´ë¥¼ ê°’ìœ¼ë¡œ `ConcurrentHashMap`ì— ì €ì¥í•©ë‹ˆë‹¤.\\n3.  í´ë¼ì´ì–¸íŠ¸ì—ì„œ SSE ì—°ê²°ì„ ì„¤ì •í•˜ê³ , ì„œë²„ë¡œë¶€í„° ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•©ë‹ˆë‹¤.\\n4.  ì‚¬ìš©ìê°€ ì•ŒëŒ êµ¬ë…/ì·¨ì†Œ ìš”ì²­ì„ í•˜ë©´, í•´ë‹¹ ì‚¬ìš©ìì˜ `SseEmitter`ë¥¼ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.\\n\\n## 5. ë¬¸ì œ í•´ê²° ê³¼ì •\\n\\n*   ì²˜ìŒì—ëŠ” `SseEmitter`ì˜ ìƒëª…ì£¼ê¸°ë¥¼ ì œëŒ€ë¡œ ê´€ë¦¬í•˜ì§€ ëª»í•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.  `SseEmitter` ê°ì²´ì˜ `close()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.\\n*   ì‚¬ìš©ì ID ì¤‘ë³µ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ëŠ” `ConcurrentHashMap`ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\\n*   í´ë¼ì´ì–¸íŠ¸ì—ì„œ SSE ì—°ê²°ì„ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•˜ê¸° ìœ„í•´ ì—ëŸ¬ í•¸ë“¤ë§ ë¡œì§ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\\n\\n## 6. í•˜ë£¨ íšŒê³ \\n\\nì˜¤ëŠ˜ì€ ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì¤‘ìš”í•œ ê¸°ìˆ ì¸ `SseEmitter`ë¥¼ ìµíˆê³  ì ìš©í•˜ëŠ” ì‹œê°„ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤.  `SseEmitter`ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³ , ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ë©´ì„œ ë§ì€ ì–´ë ¤ì›€ì„ ê²ªì—ˆì§€ë§Œ, ê²°êµ­ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œëŠ” `SseEmitter`ë¥¼ ë” ê¹Šì´ ì´í•´í•˜ê³ , ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì— ì ìš©í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤.\\n\\n## 7. ì „ì²´ì ìœ¼ë¡œ ê°œì¡°ì‹ ë¬¸ì¥ êµ¬ì„±\\n\\n*   **ëª©í‘œ:** ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ ì‹¤ì‹œê°„ì„± ë° íš¨ìœ¨ì„± í–¥ìƒ\\n*   **í•µì‹¬ ê¸°ìˆ :** SSE, SseEmitter, ConcurrentHashMap\\n*   **êµ¬í˜„ ë‹¨ê³„:**\\n    *   `SseEmitter` ê°ì²´ ìƒì„± ë° ì´ˆê¸°í™”\\n    *   ì‚¬ìš©ìë³„ `SseEmitter` ì €ì¥ ë° ê´€ë¦¬ (`ConcurrentHashMap`) \\n    *   SSE ì—°ê²° ì„¤ì • ë° ë°ì´í„° ìˆ˜ì‹ \\n    *   ì•ŒëŒ êµ¬ë…/ì·¨ì†Œ ìš”ì²­ ì²˜ë¦¬\\n    *   ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë° ì—ëŸ¬ í•¸ë“¤ë§\\n\\n\"\n",
    "}\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸°ìˆ  í•™ìŠµ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, í•˜ë‚˜ì˜ ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬,\n",
    "- ë ˆë²¨: {level}\n",
    "- ì‚¬ìš©ì TIL: {til}\n",
    "- ì°¸ê³  ë¬¸ì„œ {retrieved}\n",
    "\n",
    "â€» levelì— ë”°ë¼ ì§ˆë¬¸ ìˆ˜ì¤€ì„ ì¡°ì ˆí•´ì„œ ë©´ì ‘ ì§ˆë¬¸ \"\"1ê°œ\"\"ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n",
    "- level \"1\": ê¹Šì€ ê¸°ìˆ  ì´í•´ì™€ ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ ì§ˆë¬¸\n",
    "- level \"2\": ê°œë…ì  ì´í•´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸\n",
    "- level \"3\": ê¸°ë³¸ ê°œë…ì„ ë¬»ëŠ” ì§ˆë¬¸\n",
    "\n",
    "ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ë§Œë“¤ê³  **í•œêµ­ì–´**ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ í•˜ì§€ë§ˆì„¸ìš”.\n",
    "ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ \"\"í˜•ì‹\"\"ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”\n",
    "ì§ˆë¬¸: \"ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ? ì´ëŸ¬í•œ êµ¬ì„±ì€ ë¬´ìŠ¨ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì…ë‹ˆê¹Œ?\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51a71449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class ContentState(BaseModel):\n",
    "    question: str\n",
    "    answer: str \n",
    "\n",
    "class QAState(BaseModel):\n",
    "    email: str\n",
    "    date: str\n",
    "    level: int\n",
    "    title: str\n",
    "    keywords: List[str]\n",
    "    til: str\n",
    "\n",
    "    retrieved_texts: Optional[List[str]] = None\n",
    "    similarity_score: Optional[float] = None\n",
    "\n",
    "    question0: Optional[str] = None\n",
    "    question1: Optional[str] = None\n",
    "    question2: Optional[str] = None\n",
    "\n",
    "    question: Optional[str] = None\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "    content0: Optional[ContentState] = None\n",
    "    content1: Optional[ContentState] = None\n",
    "    content2: Optional[ContentState] = None\n",
    "\n",
    "    #output\n",
    "    content: Optional[List[ContentState]] = None\n",
    "    summary: Optional[str] = None\n",
    "\n",
    "\n",
    "qa_input = QAState(\n",
    "    email=dummy['email'],\n",
    "    date=dummy['date'],\n",
    "    level=dummy['level'],\n",
    "    title=dummy['title'],\n",
    "    keywords=dummy['keywords'],\n",
    "    til=dummy['til']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e018ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity_score': 0.52782375, 'retrieved_texts': ['ì„œë²„ ì „ì†¡ ì´ë²¤íŠ¸(Server-sent events, SSE)ëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ HTTP ì—°ê²°ì„ í†µí•´ ì„œë²„ë¡œë¶€í„° ìë™ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜ì‹ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì„œë²„ í‘¸ì‹œ ê¸°ìˆ ì´ë©°, ì´ˆê¸° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ì„¤ì •ëœ í›„ ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ë¥¼ í–¥í•œ ë°ì´í„° ì „ì†¡ì„ ì‹œì‘í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¸Œë¼ìš°ì € í´ë¼ì´ì–¸íŠ¸ì— ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ì§€ì†ì ì¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ë³´ë‚´ëŠ” ë° ì‚¬ìš©ë˜ë©° í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ìˆ˜ì‹ í•˜ê¸° ìœ„í•´ íŠ¹ì • URLì„ ìš”ì²­í•˜ëŠ” EventSourceë¼ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ APIë¥¼ í†µí•´ ê¸°ë³¸ ë¸Œë¼ìš°ì € ê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ í–¥ìƒì‹œí‚¤ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. EventSource APIëŠ” WHATWGì— ì˜í•´ HTML Living Standardì˜ ì¼ë¶€ë¡œ í‘œì¤€í™”ë˜ì—ˆë‹¤. SSEì˜ ë¯¸ë””ì–´ ìœ í˜•ì€ text/event-streamì´ë‹¤. íŒŒì´ì–´í­ìŠ¤ 6+, êµ¬ê¸€ í¬ë¡¬ 6+, ì˜¤í˜ë¼ 11.5+, ì‚¬íŒŒë¦¬ 5+, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ì—£ì§€ 79+ ë“± ëª¨ë“  ìµœì‹  ë¸Œë¼ìš°ì €ëŠ” ì„œë²„ì—ì„œ ì „ì†¡ë˜ëŠ” ì´ë²¤íŠ¸ë¥¼ ì§€ì›í•œë‹¤.', 'í‘¸ì‹œ ê¸°ë²• ê¸°ìˆ ì€ í’€ ê¸°ë²•ìœ¼ë¡œ ëª…ëª…ëœ ì›¹ë¸Œë¼ìš°ì €ì™€ ë¹„êµí•  ìˆ˜ ìˆë‹¤. í˜„ì¬ ì¸í„°ë„·ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ì›¹ë¸Œë¼ìš°ì €ëŠ” ì‚¬ìš©í•  ë•Œì— ì‚¬ìš©ìê°€ í•­ìƒ ìì‹ ì´ ê°–ê³ ì í•˜ëŠ” ì •ë³´ë¥¼ ì†Œìœ í•œ ì„œë²„ì—ê²Œ ì •ë³´ë¥¼ ìš”ì²­í•˜ë©° ì›¹ë¸Œë¼ìš°ì €ì˜ ì‚¬ìš©ê³¼ ëª©ì ì§€ì— ëŒ€í•œ ìµœì¢…ê²°ì • ì—­ì‹œ ì‚¬ìš©ìê°€ ê²°ì •í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´ í‘¸ì‹œ ê¸°ë²•ì€ ì‚¬ìš©ìê°€ ì¼ì¼ì´ ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš©ìì—ê²Œ ìë™ìœ¼ë¡œ ë‰´ìŠ¤ë‚˜ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” íŠ¹ë³„í•œ ì •ë³´, ì˜ˆë¥¼ ë“¤ë©´ ì¦ê¶Œì‹œì¥ì˜ ì£¼ê¸°ì ì¸ ì •ë³´ ê°™ì€ ê²ƒì„ ì œê³µí•œë‹¤. ì´ ë‘ ê°€ì§€ ê¸°ë²•ì˜ ì°¨ì´ì ì€ ì •ë³´ì˜ íë¦„ì„ ëˆ„ê°€ í†µì œí•˜ëŠëƒ í•˜ëŠ” ì ì— ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. í’€ ê¸°ë²• í•˜ì—ì„œëŠ” ì‚¬ìš©ì (ì¦‰ ì†Œë¹„ì)ë“¤ì´ ì •ë³´ ì·¨ë“ ë° ì •ë³´ì˜ ì ‘ì´‰ì„ ë§ˆìŒëŒ€ë¡œ í†µì œí•  ìˆ˜ ìˆìœ¼ë‚˜, í‘¸ì‹œ ê¸°ë²• í•˜ì—ì„œëŠ” ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ìª½ì—ì„œ(ì¦‰ ê´‘ê³ ì£¼)ì •ë³´ì˜ íë¦„ì„ ì§ì ‘ í†µì œí•  ìˆ˜ ìˆê²Œ ëœë‹¤.', 'í‘¸ì‹œ ê¸°ë²•ì˜ ê°€ì¥ í° ì´ì ì€ ì—­ì‹œ ì •ë³´ì˜ ë§ì¶¤í™”(Customization)ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì— ìˆë‹¤. ì¦‰ ì‚¬ìš©ìê°€ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë“±ë¡ëœ ì‚¬ìš©ì ì •ë³´ì— ì˜í•´ íƒ€ê²Ÿì„ ì •í™•íˆ ì„ ì •í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2433/2475243331.py:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vllm import SamplingParams\n",
    "from uuid import uuid4\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "result = await retriever_node(qa_input)\n",
    "qa_input.retrieved_texts = result[\"retrieved_texts\"]\n",
    "qa_input.similarity_score = result[\"similarity_score\"]\n",
    "print(result)\n",
    "\n",
    "async def question_node(state: QAState) -> dict:\n",
    "\n",
    "    retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "    prompt = prompt1.format(\n",
    "        til=state.til,\n",
    "        level=state.level,\n",
    "        retrieved=retrieved\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[2], \n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"question\": final_text\n",
    "    }\n",
    "\n",
    "# q = await question_node(qa_input)\n",
    "# qa_input.question = q[\"question\"]\n",
    "# print(q[\"question\"])\n",
    "\n",
    "async def answer_node(state: QAState) -> dict:\n",
    "    if not state.question:\n",
    "        raise ValueError(\"ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € generate_nodeë¥¼ í†µí•´ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    context = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    \n",
    "    prompt =f\"\"\"\n",
    "    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸°ìˆ  í•™ìŠµ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, í•˜ë‚˜ì˜ ê¸°ìˆ  ë©´ì ‘ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
    "\n",
    "    ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬,\n",
    "    ì§ˆë¬¸: {state.question}\n",
    "    ì‚¬ìš©ì TIL: {state.til}\n",
    "    level: {state.level}\n",
    "    - ì°¸ê³  ë¬¸ì„œ {context}\n",
    "\n",
    "    â€» levelì— ë”°ë¼ ì§ˆë¬¸ ìˆ˜ì¤€ì„ ì¡°ì ˆí•´ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ \"\"1ê°œ\"\"ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n",
    "    - level \"1\": ê¹Šì€ ê¸°ìˆ  ì´í•´ì™€ ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ ì§ˆë¬¸\n",
    "    - level \"2\": ê°œë…ì  ì´í•´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸\n",
    "    - level \"3\": ê¸°ë³¸ ê°œë…ì„ ë¬»ëŠ” ì§ˆë¬¸\n",
    "\n",
    "    ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ë‹µë³€ë§Œ ë§Œë“¤ê³  **í•œêµ­ì–´**ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "    ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë˜, ë‹µë³€ì— ìƒê´€ì—†ëŠ” ê¸°í˜¸ë‚˜ ë¬¸ìëŠ” ë¹¼ì£¼ì„¸ìš”.\n",
    "    ë‹µë³€: \" \"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[2],\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"answer\": final_text,\n",
    "        \"content\": [\n",
    "            ContentState(\n",
    "                question=state.question,\n",
    "                answer=final_text\n",
    "            )\n",
    "        ]        \n",
    "    }\n",
    "\n",
    "# a = await answer_node(qa_input)\n",
    "# qa_input.answer = a[\"answer\"]\n",
    "# qa_input.content = a[\"content\"]\n",
    "\n",
    "# print(\"ğŸ§¾ ì§ˆë¬¸:\", qa_input.content[0].question)\n",
    "# print(\"ğŸ’¬ ë‹µë³€:\", qa_input.content[0].answer)\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    content_items = state.content or []\n",
    "\n",
    "    combined = \"\\n\".join(\n",
    "        f\"Q: {item.question}\\nA: {item.answer}\" for item in content_items\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ë‹¤ìŒì€ ë©´ì ‘ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë‚´ìš©ì„ ë³´ê³  í•µì‹¬ ì£¼ì œë¥¼ í•œ ì¤„ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.  \n",
    "    ì´ ì œëª©ì€ ê°œë°œ ë¬¸ì„œë‚˜ ê¸°ëŠ¥ ì„¤ëª…ì„œì—ì„œ ì“¸ ìˆ˜ ìˆì„ ì •ë„ë¡œ ê°„ê²°í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ì˜ˆì‹œ:\n",
    "    Q: REST APIë€ ë¬´ì—‡ì¸ê°€ìš”?  \n",
    "    A: REST APIëŠ” HTTP í”„ë¡œí† ì½œì„ ê¸°ë°˜ìœ¼ë¡œ ìì›ì„ URIë¡œ í‘œí˜„í•˜ê³ , CRUDë¥¼ HTTP ë©”ì„œë“œë¡œ ìˆ˜í–‰í•˜ëŠ” ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.  \n",
    "    â†’ ì œëª©: REST API ê°œë… ë° êµ¬ì„± ìš”ì†Œ\n",
    "\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì°¸ê³ í•´ì„œ ìœ„ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ **15ê¸€ì** ì´ë‚´ë¡œ ì œëª©ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "    {combined}\n",
    "\n",
    "    â†’ ì œëª©:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.3,\n",
    "        max_tokens=32,\n",
    "        stop_token_ids=[2]\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"summary\": final_text \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43299b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 09:49:26 [async_llm.py:228] Added request 055832f9-c1b2-4b14-9a68-c35b26107530.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2433/2708729159.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 09:49:40 [async_llm.py:228] Added request c2bd692e-1409-43c3-898f-b9081ba77187.\n",
      "INFO 05-12 09:50:42 [async_llm.py:228] Added request d46e1a9a-c8dc-49e1-8d79-bce1fed8cbd3.\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# ì‹œì‘ ì§€ì  ì„¤ì •\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "# ë…¸ë“œ ì„ ì–¸\n",
    "workflow.add_node(\"retriever\",retriever_node)\n",
    "\n",
    "workflow.add_node(\"question_generate\",question_node)\n",
    "workflow.add_node(\"answer_generate\",answer_node)\n",
    "\n",
    "workflow.add_node(\"summary_generate\",summary_node)\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question_generate\")\n",
    "workflow.add_edge(\"question_generate\", \"answer_generate\")\n",
    "workflow.add_edge(\"answer_generate\", \"summary_generate\")\n",
    "\n",
    "# ì¢…ë£Œ ì§€ì  ì„¤ì •\n",
    "workflow.set_finish_point(\"summary_generate\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee0027",
   "metadata": {},
   "source": [
    "### í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "824caad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class QAFlow:\n",
    "    def __init__(self, llm, qdrant, prompt_template, max_nodes=3):\n",
    "        self.llm = llm\n",
    "        self.qdrant = qdrant\n",
    "        self.prompt1 = prompt_template\n",
    "        self.max_nodes = max_nodes\n",
    "        self.embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "    def embed_text(self, text: str) -> list[float]:\n",
    "        return self.embedding_model.encode(text).tolist()\n",
    "\n",
    "    async def retriever_node(self, state: QAState) -> dict:\n",
    "        query = state.title + \" \" + \" \".join(state.keywords)\n",
    "        query_vector = self.embed_text(query)\n",
    "\n",
    "        collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "        best_score = 0.0\n",
    "        retrieved_texts = []\n",
    "\n",
    "        for col in collection_names:\n",
    "            results = self.qdrant.search(\n",
    "                collection_name=col,\n",
    "                query_vector=query_vector,\n",
    "                limit=3,\n",
    "                with_payload=True\n",
    "            )\n",
    "            if results and results[0].score > best_score:\n",
    "                best_score = results[0].score\n",
    "                retrieved_texts = [r.payload[\"text\"] for r in results if \"text\" in r.payload]\n",
    "\n",
    "        return {\n",
    "            \"similarity_score\": best_score,\n",
    "            \"retrieved_texts\": retrieved_texts\n",
    "        }\n",
    "\n",
    "    def generate_question_node(self, node_id: int):\n",
    "        async def question_node(state: QAState) -> dict:\n",
    "            retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "            \n",
    "            prompt = self.prompt1.format(\n",
    "                til=state.til,\n",
    "                level=state.level,\n",
    "                retrieved=retrieved\n",
    "            )\n",
    "\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                max_tokens=1024,\n",
    "                stop_token_ids=[2]\n",
    "            )\n",
    "\n",
    "            request_id = str(uuid4())\n",
    "            final_text = \"\"\n",
    "\n",
    "            async for output in self.llm.generate(\n",
    "                prompt=prompt,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id\n",
    "            ):\n",
    "                final_text = output.outputs[0].text.strip()\n",
    "\n",
    "            return {f\"question{node_id}\": final_text}\n",
    "\n",
    "        return question_node\n",
    "\n",
    "    def generate_answer_node(self, node_id: int):\n",
    "        async def answer_node(state: QAState) -> dict:\n",
    "            question = getattr(state, f\"question{node_id}\", None)\n",
    "            if not question:\n",
    "                raise ValueError(f\"ì§ˆë¬¸ {node_id}ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "            context = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            ì§ˆë¬¸: {question}\n",
    "            ì‚¬ìš©ì TIL: {state.til}\n",
    "            level: {state.level}\n",
    "            ì°¸ê³  ë¬¸ì„œ: {context}\n",
    "\n",
    "            ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ë‹µë³€ í•˜ë‚˜ë§Œ ìƒì„±í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ëŠ” ì œê±°í•˜ê³ , ê°„ê²°í•˜ê²Œ ì„œìˆ í•´ì£¼ì„¸ìš”.\n",
    "            \"\"\"\n",
    "\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                max_tokens=1024,\n",
    "                stop_token_ids=[2]\n",
    "            )\n",
    "\n",
    "            request_id = str(uuid4())\n",
    "            final_text = \"\"\n",
    "\n",
    "            async for output in self.llm.generate(\n",
    "                prompt=prompt,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id\n",
    "            ):\n",
    "                final_text = output.outputs[0].text.strip()\n",
    "\n",
    "            return {\n",
    "                #\"content\": [ContentState(question=question, answer=final_text)]\n",
    "                f\"content{node_id}\": ContentState(\n",
    "                    question=question,\n",
    "                    answer=final_text)\n",
    "            }\n",
    "\n",
    "        return answer_node\n",
    "\n",
    "    async def summary_node(self, state: QAState) -> dict:\n",
    "        merged = []\n",
    "        for i in range(3):\n",
    "            item = getattr(state, f\"content{i}\", None)\n",
    "            if item:\n",
    "                merged.append(item)\n",
    "\n",
    "        combined = \"\\n\".join(\n",
    "            f\"Q: {item.question}\\nA: {item.answer}\" for item in merged\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¤ìŒì€ ë©´ì ‘ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ë‚´ìš©ì„ ë³´ê³  í•µì‹¬ ì£¼ì œë¥¼ í•œ ì¤„ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.  \n",
    "        ì´ ì œëª©ì€ ê°œë°œ ë¬¸ì„œë‚˜ ê¸°ëŠ¥ ì„¤ëª…ì„œì—ì„œ ì“¸ ìˆ˜ ìˆì„ ì •ë„ë¡œ ê°„ê²°í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "        ì˜ˆì‹œ:\n",
    "        Q: REST APIë€ ë¬´ì—‡ì¸ê°€ìš”?  \n",
    "        A: REST APIëŠ” HTTP í”„ë¡œí† ì½œì„ ê¸°ë°˜ìœ¼ë¡œ ìì›ì„ URIë¡œ í‘œí˜„í•˜ê³ , CRUDë¥¼ HTTP ë©”ì„œë“œë¡œ ìˆ˜í–‰í•˜ëŠ” ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.  \n",
    "        â†’ ì œëª©: REST API ê°œë… ë° êµ¬ì„± ìš”ì†Œ\n",
    "\n",
    "        ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì°¸ê³ í•´ì„œ ìœ„ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ **15ê¸€ì** ì´ë‚´ë¡œ ì œëª©ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "        {combined}\n",
    "\n",
    "        â†’ ì œëª©:\n",
    "        \"\"\"\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.3,\n",
    "            max_tokens=32,\n",
    "            stop_token_ids=[2]\n",
    "        )\n",
    "\n",
    "        request_id = str(uuid4())\n",
    "        final_text = \"\"\n",
    "\n",
    "        async for output in self.llm.generate(\n",
    "            prompt=prompt,\n",
    "            sampling_params=sampling_params,\n",
    "            request_id=request_id\n",
    "        ):\n",
    "            final_text = output.outputs[0].text.strip()\n",
    "\n",
    "        return {\n",
    "            \"summary\": final_text,\n",
    "            \"content\": merged\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        workflow = StateGraph(QAState)\n",
    "        workflow.set_entry_point(\"retriever\")\n",
    "        workflow.add_node(\"retriever\", self.retriever_node)\n",
    "\n",
    "        for i in range(self.max_nodes):\n",
    "            workflow.add_node(f\"que{i}\", self.generate_question_node(i))\n",
    "            workflow.add_node(f\"ans{i}\", self.generate_answer_node(i))\n",
    "\n",
    "            workflow.add_edge(\"retriever\", f\"que{i}\")\n",
    "            workflow.add_edge(f\"que{i}\", f\"ans{i}\")\n",
    "            workflow.add_edge(f\"ans{i}\", \"summary_generate\")\n",
    "\n",
    "        workflow.add_node(\"summary_generate\", self.summary_node)\n",
    "        workflow.set_finish_point(\"summary_generate\")\n",
    "\n",
    "        return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0a1d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 11:11:31 [async_llm.py:228] Added request 096bd9ff-d88b-45f5-b55f-3c935e44d7e0.\n",
      "INFO 05-12 11:11:31 [async_llm.py:228] Added request 133ab610-069a-4f32-81bc-9e35ea5aa110.\n",
      "INFO 05-12 11:11:31 [async_llm.py:228] Added request 0f0d9614-b08d-41b5-8b8c-743766e59dae.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2433/1706059924.py:26: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 11:11:49 [async_llm.py:228] Added request 5a7bd971-95f0-4e3e-8fd1-ab85f3849ad4.\n",
      "INFO 05-12 11:11:49 [async_llm.py:228] Added request 51783931-a23a-4e34-ae8e-4af1a4390d70.\n",
      "INFO 05-12 11:11:49 [async_llm.py:228] Added request e6c78953-2670-4e4e-a613-49fd4c8c5e3d.\n",
      "INFO 05-12 11:12:42 [async_llm.py:228] Added request 14ab5162-4d61-4d92-9550-c27fe1732840.\n",
      "Summary: í‘¸ì‹œ ê¸°ìˆ  vs í’€ ê¸°ìˆ : ìë™ ì •ë³´ ì œê³µ,\n",
      "ì§ˆë¬¸: ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ? ì´ëŸ¬í•œ êµ¬ì„±ì€ ë¬´ìŠ¨ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì…ë‹ˆê¹Œ?\n",
      "ì¦‰, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SSEì™€ `SseEmitter`, `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ì‚¬ìš© ìƒí™©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "ë‹µë³€: ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ëŠ” SSE(Server-Sent Events)ì…ë‹ˆë‹¤. SSEëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ HTTP ì—°ê²°ì„ í†µí•´ ì„œë²„ë¡œë¶€í„° ìë™ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ëŠ” ì„œë²„ í‘¸ì‹œ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ í™œìš©í•˜ë©´ ì„œë²„ê°€ ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì•ŒëŒì„ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SSEëŠ” ìµœì‹  ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì§€ì›í•˜ë©°, ì´ë¥¼ ìœ„í•´ EventSourceë¼ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "            ì´ëŸ¬í•œ ê¸°ìˆ ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `SseEmitter`ëŠ” ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, `ConcurrentHashMap`ì€ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ì ‘ê·¼í•´ë„ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤. ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ, `SseEmitter`ë¥¼ ê°’ìœ¼ë¡œ `ConcurrentHashMap`ì— ì €ì¥í•˜ì—¬ ê° ì‚¬ìš©ìì— ëŒ€í•œ `SseEmitter`ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•˜ë©´ í•´ë‹¹ ì‚¬ìš©ìì˜ `SseEmitter`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì‚¬ìš©ìì—ê²Œë§Œ ì•ŒëŒì„ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "            ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ëª‡ ê°€ì§€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒìœ¼ë¡œ ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í´ë¼ì´ì–¸íŠ¸ëŠ” ì •ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ, ì„œë²„ ìì›ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì•ŒëŒì„ ì „ì†¡í•˜ë¯€ë¡œ, ì‹¤ì‹œê°„ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ìœ ìš©í•œ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ì§ˆë¬¸: \"ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ?\"\n",
      "\n",
      "ì´ êµ¬ì„±ì—ëŠ” ì•ŒëŒ ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ì„±ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ê¸°ì¡´ì˜ ì•ŒëŒ êµ¬ë… ë°©ì‹ì— ë¹„í•´ ì„œë²„ ë¶€í•˜ê°€ ê°ì†Œí•˜ê³ , ì‹¤ì‹œê°„ì„±ì´ ë†’ì•„ì§€ëŠ” ì´ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‚¬ìš©ìë³„ë¡œ SseEmitterë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ë‹µë³€: ì‹¤ì‹œê°„ ì•Œë¦¼ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•©ë‹ˆë‹¤. SseEmitterëŠ” ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆê³ , ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ConcurrentHashMapì€ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤. ì‚¬ìš©ìë³„ SseEmitterë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ì—¬ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•Œë¦¼ë§Œ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë”ìš± íš¨ìœ¨ì ì¸ ì•Œë¦¼ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: ì§ˆë¬¸: \"í‘¸ì‹œ ê¸°ë²•ì´ í’€ ê¸°ë²•ê³¼ ì–´ë–¤ ì°¨ì´ì ì´ ìˆìŠµë‹ˆê¹Œ? í‘¸ì‹œ ê¸°ë²•ì˜ ì¥ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´ë–¤ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆê¹Œ?\n",
      "ë‹µë³€: í•´ë‹¹ ê¸°ìˆ ì€ í’€ ê¸°ë²•ê³¼ì˜ ì°¨ì´ì ìœ¼ë¡œ ì¸í„°ë„·ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ì›¹ ë¸Œë¼ìš°ì €ì™€ ë‹¤ë¥´ê²Œ ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë‰´ìŠ¤ë‚˜ íŠ¹ë³„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” í‘¸ì‹œ ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì˜ ê°€ì¥ í° ì¥ì ì€ ì •ë³´ì˜ ë§ì¶¤í™”, ì¦‰, ë“±ë¡ëœ ì‚¬ìš©ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒ€ê²Ÿì„ ì •í™•íˆ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_flow = QAFlow(llm=llm, qdrant=qdrant, prompt_template=prompt1, max_nodes=3)\n",
    "graph = qa_flow.build_graph()\n",
    "result = await graph.ainvoke(qa_input)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Summary:\", result[\"summary\"])\n",
    "\n",
    "for c in result[\"content\"]:\n",
    "    print(f\"ì§ˆë¬¸: {c.question}\")\n",
    "    print(f\"ë‹µë³€: {c.answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0f89f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretriever(retriever)\n",
      "\tque0(que0)\n",
      "\tans0(ans0)\n",
      "\tque1(que1)\n",
      "\tans1(ans1)\n",
      "\tque2(que2)\n",
      "\tans2(ans2)\n",
      "\tsummary_generate(summary_generate)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retriever;\n",
      "\tans0 --> summary_generate;\n",
      "\tans1 --> summary_generate;\n",
      "\tans2 --> summary_generate;\n",
      "\tque0 --> ans0;\n",
      "\tque1 --> ans1;\n",
      "\tque2 --> ans2;\n",
      "\tretriever --> que0;\n",
      "\tretriever --> que1;\n",
      "\tretriever --> que2;\n",
      "\tsummary_generate --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_flow = QAFlow(llm=llm, qdrant=qdrant, prompt_template=prompt1, max_nodes=3)\n",
    "graph = qa_flow.build_graph()\n",
    "\n",
    "# ì‹œê°í™” ì½”ë“œ\n",
    "print(graph.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c4dcfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 'ConconDev',\n",
       " 'date': '2024-09-06',\n",
       " 'level': 1,\n",
       " 'title': 'ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ ê°œì„ : SseEmitter í™œìš© ë° ì‚¬ìš©ìë³„ Emitter ê´€ë¦¬',\n",
       " 'keywords': ['SSE', 'SseEmitter', 'ì•ŒëŒ êµ¬ë…', 'ì‚¬ìš©ìë³„ ê´€ë¦¬'],\n",
       " 'til': '# ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ ê°œì„ : SseEmitter í™œìš© ë° ì‚¬ìš©ìë³„ Emitter ê´€ë¦¬\\n\\n## 1. ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©\\n\\nì˜¤ëŠ˜ ì €ëŠ” ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ë¥¼ í™œìš©í•˜ê³ , ê° ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.  `SseEmitter`ëŠ” ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë° ìœ ìš©í•œ APIì…ë‹ˆë‹¤.\\n\\n## 2. ê°œë… ì •ë¦¬\\n\\n*   **SSE (Server-Sent Events):** ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë‹¨ë°©í–¥ í†µì‹ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì›¹ ê¸°ìˆ ì…ë‹ˆë‹¤. ì„œë²„ê°€ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë°œìƒ ì‹œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.\\n*   **SseEmitter:** SSE í†µì‹ ì„ ìœ„í•œ ê°ì²´ì…ë‹ˆë‹¤.  ë°ì´í„°ë¥¼ ë°œí–‰í•˜ê±°ë‚˜, ì—°ê²°ì„ ì¢…ë£Œí•˜ê±°ë‚˜, ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\\n*   **ConcurrentHashMap:** ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ì ‘ê·¼í•´ë„ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤.  ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ í•˜ê³  `SseEmitter`ë¥¼ ê°’ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\n## 3. í•´ë‹¹ ê°œë…ì´ í•„ìš”í•œ ì´ìœ \\n\\nê¸°ì¡´ ì•ŒëŒ êµ¬ë… ë°©ì‹ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ë°©ì‹ìœ¼ë¡œ, ì„œë²„ ë¶€í•˜ê°€ ì‹¬í•˜ê³  íš¨ìœ¨ì„±ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. `SseEmitter`ë¥¼ ì‚¬ìš©í•˜ë©´ ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí–ˆì„ ë•Œë§Œ í´ë¼ì´ì–¸íŠ¸ì— ë°ì´í„°ë¥¼ ì „ì†¡í•˜ë¯€ë¡œ, ì„œë²„ ìì›ì„ ì ˆì•½í•˜ê³  ì‹¤ì‹œê°„ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨, íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n## 4. ê°œë…ì„ í™œìš©í•˜ëŠ” ë°©ë²•\\n\\n1.  `SseEmitter` ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\\n2.  ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ, `SseEmitter` ê°ì²´ë¥¼ ê°’ìœ¼ë¡œ `ConcurrentHashMap`ì— ì €ì¥í•©ë‹ˆë‹¤.\\n3.  í´ë¼ì´ì–¸íŠ¸ì—ì„œ SSE ì—°ê²°ì„ ì„¤ì •í•˜ê³ , ì„œë²„ë¡œë¶€í„° ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•©ë‹ˆë‹¤.\\n4.  ì‚¬ìš©ìê°€ ì•ŒëŒ êµ¬ë…/ì·¨ì†Œ ìš”ì²­ì„ í•˜ë©´, í•´ë‹¹ ì‚¬ìš©ìì˜ `SseEmitter`ë¥¼ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.\\n\\n## 5. ë¬¸ì œ í•´ê²° ê³¼ì •\\n\\n*   ì²˜ìŒì—ëŠ” `SseEmitter`ì˜ ìƒëª…ì£¼ê¸°ë¥¼ ì œëŒ€ë¡œ ê´€ë¦¬í•˜ì§€ ëª»í•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.  `SseEmitter` ê°ì²´ì˜ `close()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.\\n*   ì‚¬ìš©ì ID ì¤‘ë³µ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ëŠ” `ConcurrentHashMap`ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\\n*   í´ë¼ì´ì–¸íŠ¸ì—ì„œ SSE ì—°ê²°ì„ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•˜ê¸° ìœ„í•´ ì—ëŸ¬ í•¸ë“¤ë§ ë¡œì§ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\\n\\n## 6. í•˜ë£¨ íšŒê³ \\n\\nì˜¤ëŠ˜ì€ ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì¤‘ìš”í•œ ê¸°ìˆ ì¸ `SseEmitter`ë¥¼ ìµíˆê³  ì ìš©í•˜ëŠ” ì‹œê°„ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤.  `SseEmitter`ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³ , ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ë©´ì„œ ë§ì€ ì–´ë ¤ì›€ì„ ê²ªì—ˆì§€ë§Œ, ê²°êµ­ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œëŠ” `SseEmitter`ë¥¼ ë” ê¹Šì´ ì´í•´í•˜ê³ , ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì— ì ìš©í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤.\\n\\n## 7. ì „ì²´ì ìœ¼ë¡œ ê°œì¡°ì‹ ë¬¸ì¥ êµ¬ì„±\\n\\n*   **ëª©í‘œ:** ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ ì‹¤ì‹œê°„ì„± ë° íš¨ìœ¨ì„± í–¥ìƒ\\n*   **í•µì‹¬ ê¸°ìˆ :** SSE, SseEmitter, ConcurrentHashMap\\n*   **êµ¬í˜„ ë‹¨ê³„:**\\n    *   `SseEmitter` ê°ì²´ ìƒì„± ë° ì´ˆê¸°í™”\\n    *   ì‚¬ìš©ìë³„ `SseEmitter` ì €ì¥ ë° ê´€ë¦¬ (`ConcurrentHashMap`) \\n    *   SSE ì—°ê²° ì„¤ì • ë° ë°ì´í„° ìˆ˜ì‹ \\n    *   ì•ŒëŒ êµ¬ë…/ì·¨ì†Œ ìš”ì²­ ì²˜ë¦¬\\n    *   ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë° ì—ëŸ¬ í•¸ë“¤ë§\\n\\n',\n",
       " 'retrieved_texts': ['ì„œë²„ ì „ì†¡ ì´ë²¤íŠ¸(Server-sent events, SSE)ëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ HTTP ì—°ê²°ì„ í†µí•´ ì„œë²„ë¡œë¶€í„° ìë™ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜ì‹ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì„œë²„ í‘¸ì‹œ ê¸°ìˆ ì´ë©°, ì´ˆê¸° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ì„¤ì •ëœ í›„ ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ë¥¼ í–¥í•œ ë°ì´í„° ì „ì†¡ì„ ì‹œì‘í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¸Œë¼ìš°ì € í´ë¼ì´ì–¸íŠ¸ì— ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ì§€ì†ì ì¸ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ë³´ë‚´ëŠ” ë° ì‚¬ìš©ë˜ë©° í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ìˆ˜ì‹ í•˜ê¸° ìœ„í•´ íŠ¹ì • URLì„ ìš”ì²­í•˜ëŠ” EventSourceë¼ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ APIë¥¼ í†µí•´ ê¸°ë³¸ ë¸Œë¼ìš°ì € ê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ í–¥ìƒì‹œí‚¤ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. EventSource APIëŠ” WHATWGì— ì˜í•´ HTML Living Standardì˜ ì¼ë¶€ë¡œ í‘œì¤€í™”ë˜ì—ˆë‹¤. SSEì˜ ë¯¸ë””ì–´ ìœ í˜•ì€ text/event-streamì´ë‹¤. íŒŒì´ì–´í­ìŠ¤ 6+, êµ¬ê¸€ í¬ë¡¬ 6+, ì˜¤í˜ë¼ 11.5+, ì‚¬íŒŒë¦¬ 5+, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ì—£ì§€ 79+ ë“± ëª¨ë“  ìµœì‹  ë¸Œë¼ìš°ì €ëŠ” ì„œë²„ì—ì„œ ì „ì†¡ë˜ëŠ” ì´ë²¤íŠ¸ë¥¼ ì§€ì›í•œë‹¤.',\n",
       "  'í‘¸ì‹œ ê¸°ë²• ê¸°ìˆ ì€ í’€ ê¸°ë²•ìœ¼ë¡œ ëª…ëª…ëœ ì›¹ë¸Œë¼ìš°ì €ì™€ ë¹„êµí•  ìˆ˜ ìˆë‹¤. í˜„ì¬ ì¸í„°ë„·ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ì›¹ë¸Œë¼ìš°ì €ëŠ” ì‚¬ìš©í•  ë•Œì— ì‚¬ìš©ìê°€ í•­ìƒ ìì‹ ì´ ê°–ê³ ì í•˜ëŠ” ì •ë³´ë¥¼ ì†Œìœ í•œ ì„œë²„ì—ê²Œ ì •ë³´ë¥¼ ìš”ì²­í•˜ë©° ì›¹ë¸Œë¼ìš°ì €ì˜ ì‚¬ìš©ê³¼ ëª©ì ì§€ì— ëŒ€í•œ ìµœì¢…ê²°ì • ì—­ì‹œ ì‚¬ìš©ìê°€ ê²°ì •í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´ í‘¸ì‹œ ê¸°ë²•ì€ ì‚¬ìš©ìê°€ ì¼ì¼ì´ ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš©ìì—ê²Œ ìë™ìœ¼ë¡œ ë‰´ìŠ¤ë‚˜ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” íŠ¹ë³„í•œ ì •ë³´, ì˜ˆë¥¼ ë“¤ë©´ ì¦ê¶Œì‹œì¥ì˜ ì£¼ê¸°ì ì¸ ì •ë³´ ê°™ì€ ê²ƒì„ ì œê³µí•œë‹¤. ì´ ë‘ ê°€ì§€ ê¸°ë²•ì˜ ì°¨ì´ì ì€ ì •ë³´ì˜ íë¦„ì„ ëˆ„ê°€ í†µì œí•˜ëŠëƒ í•˜ëŠ” ì ì— ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. í’€ ê¸°ë²• í•˜ì—ì„œëŠ” ì‚¬ìš©ì (ì¦‰ ì†Œë¹„ì)ë“¤ì´ ì •ë³´ ì·¨ë“ ë° ì •ë³´ì˜ ì ‘ì´‰ì„ ë§ˆìŒëŒ€ë¡œ í†µì œí•  ìˆ˜ ìˆìœ¼ë‚˜, í‘¸ì‹œ ê¸°ë²• í•˜ì—ì„œëŠ” ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ìª½ì—ì„œ(ì¦‰ ê´‘ê³ ì£¼)ì •ë³´ì˜ íë¦„ì„ ì§ì ‘ í†µì œí•  ìˆ˜ ìˆê²Œ ëœë‹¤.',\n",
       "  'í‘¸ì‹œ ê¸°ë²•ì˜ ê°€ì¥ í° ì´ì ì€ ì—­ì‹œ ì •ë³´ì˜ ë§ì¶¤í™”(Customization)ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì— ìˆë‹¤. ì¦‰ ì‚¬ìš©ìê°€ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë“±ë¡ëœ ì‚¬ìš©ì ì •ë³´ì— ì˜í•´ íƒ€ê²Ÿì„ ì •í™•íˆ ì„ ì •í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.'],\n",
       " 'similarity_score': 0.52782375,\n",
       " 'question0': 'ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ? ì´ëŸ¬í•œ êµ¬ì„±ì€ ë¬´ìŠ¨ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì…ë‹ˆê¹Œ?\\nì¦‰, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SSEì™€ `SseEmitter`, `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ì‚¬ìš© ìƒí™©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.',\n",
       " 'question1': 'ì§ˆë¬¸: \"ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ?\"\\n\\nì´ êµ¬ì„±ì—ëŠ” ì•ŒëŒ ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ì„±ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ê¸°ì¡´ì˜ ì•ŒëŒ êµ¬ë… ë°©ì‹ì— ë¹„í•´ ì„œë²„ ë¶€í•˜ê°€ ê°ì†Œí•˜ê³ , ì‹¤ì‹œê°„ì„±ì´ ë†’ì•„ì§€ëŠ” ì´ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‚¬ìš©ìë³„ë¡œ SseEmitterë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',\n",
       " 'question2': 'ì§ˆë¬¸: \"í‘¸ì‹œ ê¸°ë²•ì´ í’€ ê¸°ë²•ê³¼ ì–´ë–¤ ì°¨ì´ì ì´ ìˆìŠµë‹ˆê¹Œ? í‘¸ì‹œ ê¸°ë²•ì˜ ì¥ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´ë–¤ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆê¹Œ?',\n",
       " 'content0': ContentState(question='ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ? ì´ëŸ¬í•œ êµ¬ì„±ì€ ë¬´ìŠ¨ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì…ë‹ˆê¹Œ?\\nì¦‰, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SSEì™€ `SseEmitter`, `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ì‚¬ìš© ìƒí™©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.', answer='ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ëŠ” SSE(Server-Sent Events)ì…ë‹ˆë‹¤. SSEëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ HTTP ì—°ê²°ì„ í†µí•´ ì„œë²„ë¡œë¶€í„° ìë™ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ëŠ” ì„œë²„ í‘¸ì‹œ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ í™œìš©í•˜ë©´ ì„œë²„ê°€ ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì•ŒëŒì„ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SSEëŠ” ìµœì‹  ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì§€ì›í•˜ë©°, ì´ë¥¼ ìœ„í•´ EventSourceë¼ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n\\n            ì´ëŸ¬í•œ ê¸°ìˆ ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `SseEmitter`ëŠ” ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, `ConcurrentHashMap`ì€ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ì ‘ê·¼í•´ë„ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤. ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ, `SseEmitter`ë¥¼ ê°’ìœ¼ë¡œ `ConcurrentHashMap`ì— ì €ì¥í•˜ì—¬ ê° ì‚¬ìš©ìì— ëŒ€í•œ `SseEmitter`ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•˜ë©´ í•´ë‹¹ ì‚¬ìš©ìì˜ `SseEmitter`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì‚¬ìš©ìì—ê²Œë§Œ ì•ŒëŒì„ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n            ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ëª‡ ê°€ì§€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒìœ¼ë¡œ ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í´ë¼ì´ì–¸íŠ¸ëŠ” ì •ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ, ì„œë²„ ìì›ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì•ŒëŒì„ ì „ì†¡í•˜ë¯€ë¡œ, ì‹¤ì‹œê°„ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ìœ ìš©í•œ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤.'),\n",
       " 'content1': ContentState(question='ì§ˆë¬¸: \"ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ?\"\\n\\nì´ êµ¬ì„±ì—ëŠ” ì•ŒëŒ ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ì„±ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ê¸°ì¡´ì˜ ì•ŒëŒ êµ¬ë… ë°©ì‹ì— ë¹„í•´ ì„œë²„ ë¶€í•˜ê°€ ê°ì†Œí•˜ê³ , ì‹¤ì‹œê°„ì„±ì´ ë†’ì•„ì§€ëŠ” ì´ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‚¬ìš©ìë³„ë¡œ SseEmitterë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', answer='ì‹¤ì‹œê°„ ì•Œë¦¼ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•©ë‹ˆë‹¤. SseEmitterëŠ” ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆê³ , ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ConcurrentHashMapì€ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤. ì‚¬ìš©ìë³„ SseEmitterë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ì—¬ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•Œë¦¼ë§Œ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë”ìš± íš¨ìœ¨ì ì¸ ì•Œë¦¼ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'),\n",
       " 'content2': ContentState(question='ì§ˆë¬¸: \"í‘¸ì‹œ ê¸°ë²•ì´ í’€ ê¸°ë²•ê³¼ ì–´ë–¤ ì°¨ì´ì ì´ ìˆìŠµë‹ˆê¹Œ? í‘¸ì‹œ ê¸°ë²•ì˜ ì¥ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´ë–¤ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆê¹Œ?', answer='í•´ë‹¹ ê¸°ìˆ ì€ í’€ ê¸°ë²•ê³¼ì˜ ì°¨ì´ì ìœ¼ë¡œ ì¸í„°ë„·ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ì›¹ ë¸Œë¼ìš°ì €ì™€ ë‹¤ë¥´ê²Œ ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë‰´ìŠ¤ë‚˜ íŠ¹ë³„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” í‘¸ì‹œ ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì˜ ê°€ì¥ í° ì¥ì ì€ ì •ë³´ì˜ ë§ì¶¤í™”, ì¦‰, ë“±ë¡ëœ ì‚¬ìš©ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒ€ê²Ÿì„ ì •í™•íˆ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.'),\n",
       " 'content': [ContentState(question='ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ? ì´ëŸ¬í•œ êµ¬ì„±ì€ ë¬´ìŠ¨ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì…ë‹ˆê¹Œ?\\nì¦‰, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SSEì™€ `SseEmitter`, `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ì™€ ì‚¬ìš© ìƒí™©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.', answer='ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ëŠ” SSE(Server-Sent Events)ì…ë‹ˆë‹¤. SSEëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ HTTP ì—°ê²°ì„ í†µí•´ ì„œë²„ë¡œë¶€í„° ìë™ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ëŠ” ì„œë²„ í‘¸ì‹œ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ í™œìš©í•˜ë©´ ì„œë²„ê°€ ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì•ŒëŒì„ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SSEëŠ” ìµœì‹  ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì§€ì›í•˜ë©°, ì´ë¥¼ ìœ„í•´ EventSourceë¼ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n\\n            ì´ëŸ¬í•œ ê¸°ìˆ ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `SseEmitter`ëŠ” ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, `ConcurrentHashMap`ì€ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ì ‘ê·¼í•´ë„ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤. ì‚¬ìš©ì IDë¥¼ í‚¤ë¡œ, `SseEmitter`ë¥¼ ê°’ìœ¼ë¡œ `ConcurrentHashMap`ì— ì €ì¥í•˜ì—¬ ê° ì‚¬ìš©ìì— ëŒ€í•œ `SseEmitter`ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•˜ë©´ í•´ë‹¹ ì‚¬ìš©ìì˜ `SseEmitter`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì‚¬ìš©ìì—ê²Œë§Œ ì•ŒëŒì„ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n            ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ëª‡ ê°€ì§€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒìœ¼ë¡œ ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í´ë¼ì´ì–¸íŠ¸ëŠ” ì •ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ, ì„œë²„ ìì›ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì„œë²„ëŠ” ìƒˆë¡œìš´ ì•ŒëŒì´ ë°œìƒí•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì•ŒëŒì„ ì „ì†¡í•˜ë¯€ë¡œ, ì‹¤ì‹œê°„ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìë³„ë¡œ `SseEmitter`ë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ `SseEmitter`ì™€ `ConcurrentHashMap`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ìœ ìš©í•œ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤.'),\n",
       "  ContentState(question='ì§ˆë¬¸: \"ì•ŒëŒ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ì´ëŸ¬í•œ êµ¬ì„±ì—ëŠ” ì–´ë–¤ ì¥ì ì´ ìˆìŠµë‹ˆê¹Œ?\"\\n\\nì´ êµ¬ì„±ì—ëŠ” ì•ŒëŒ ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ì„±ì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ê¸°ì¡´ì˜ ì•ŒëŒ êµ¬ë… ë°©ì‹ì— ë¹„í•´ ì„œë²„ ë¶€í•˜ê°€ ê°ì†Œí•˜ê³ , ì‹¤ì‹œê°„ì„±ì´ ë†’ì•„ì§€ëŠ” ì´ì ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‚¬ìš©ìë³„ë¡œ SseEmitterë¥¼ ê´€ë¦¬í•¨ìœ¼ë¡œì¨ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•ŒëŒë§Œ ì „ì†¡í•˜ë„ë¡ í•  ìˆ˜ ìˆì–´ ë”ìš± íš¨ìœ¨ì ì¸ ì•ŒëŒ êµ¬ë… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', answer='ì‹¤ì‹œê°„ ì•Œë¦¼ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ SseEmitterì™€ ConcurrentHashMapì„ ì‚¬ìš©í•©ë‹ˆë‹¤. SseEmitterëŠ” ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆê³ , ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ConcurrentHashMapì€ ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í•´ì‹œë§µì…ë‹ˆë‹¤. ì‚¬ìš©ìë³„ SseEmitterë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ì—¬ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì•Œë¦¼ë§Œ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë”ìš± íš¨ìœ¨ì ì¸ ì•Œë¦¼ êµ¬ë… ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'),\n",
       "  ContentState(question='ì§ˆë¬¸: \"í‘¸ì‹œ ê¸°ë²•ì´ í’€ ê¸°ë²•ê³¼ ì–´ë–¤ ì°¨ì´ì ì´ ìˆìŠµë‹ˆê¹Œ? í‘¸ì‹œ ê¸°ë²•ì˜ ì¥ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? ì´ëŸ¬í•œ ê¸°ìˆ ì€ ì–´ë–¤ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆê¹Œ?', answer='í•´ë‹¹ ê¸°ìˆ ì€ í’€ ê¸°ë²•ê³¼ì˜ ì°¨ì´ì ìœ¼ë¡œ ì¸í„°ë„·ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ ì›¹ ë¸Œë¼ìš°ì €ì™€ ë‹¤ë¥´ê²Œ ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë‰´ìŠ¤ë‚˜ íŠ¹ë³„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” í‘¸ì‹œ ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì˜ ê°€ì¥ í° ì¥ì ì€ ì •ë³´ì˜ ë§ì¶¤í™”, ì¦‰, ë“±ë¡ëœ ì‚¬ìš©ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒ€ê²Ÿì„ ì •í™•íˆ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.')],\n",
       " 'summary': 'í‘¸ì‹œ ê¸°ìˆ  vs í’€ ê¸°ìˆ : ìë™ ì •ë³´ ì œê³µ,'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e569f1",
   "metadata": {},
   "source": [
    "## ë ˆê±°ì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f43b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "import os\n",
    "\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "    retrieved_texts: List[str] = []\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "prom = \"\"\"\n",
    "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸°ìˆ  í•™ìŠµ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬,\n",
    "[TIL ë³¸ë¬¸] {til}\n",
    "[RAG ê²€ìƒ‰ ê²°ê³¼] {text}\n",
    "[ì„ íƒí•œ ë‚œì´ë„] {level}\n",
    "\n",
    "â€» levelì— ë”°ë¼ ì§ˆë¬¸ ìˆ˜ì¤€ì„ ì¡°ì ˆí•´ì„œ ë©´ì ‘ ì§ˆë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n",
    "- level \"1\": ê¹Šì€ ê¸°ìˆ  ì´í•´ì™€ ì‹¤ë¬´ ê²½í—˜ ê¸°ë°˜ ì§ˆë¬¸\n",
    "- level \"2\": ê°œë…ì  ì´í•´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸\n",
    "- level \"3\": ê¸°ë³¸ ê°œë…ì„ ë¬»ëŠ” ì§ˆë¬¸\n",
    "\n",
    "ëª¨ë“  ì§ˆë¬¸ê³¼ ë‹µë³€ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "\"\"\"\n",
    "# ì§ˆë¬¸ ìƒì„± ë…¸ë“œ\n",
    "async def question0_node(state: QAState) -> Dict[str, Any]:\n",
    "    text = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    prompt = prom.format(\n",
    "        til=state.til,\n",
    "        text=text,\n",
    "        level=state.level\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(prompt, sampling_params, request_id=request_id):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    # ì§ˆë¬¸ë§Œ ì¶”ì¶œ\n",
    "    match = re.search(r\"ì§ˆë¬¸[:ï¼š]\\s*(.+)\", final_text)\n",
    "    question = match.group(1).strip() if match else \"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\"\n",
    "\n",
    "    return {\n",
    "        \"qa_0\": ContentState(question=question, answer=\"\")\n",
    "    }\n",
    "\n",
    "async def question1_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q1: '{state.title}' ê´€ë ¨ ë‘ ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\"\n",
    "    return {\"qa_1\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "async def question2_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q2: '{state.title}' ê´€ë ¨ ì„¸ ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.\"\n",
    "    return {\"qa_2\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "async def answer0_node(state: QAState) -> dict:\n",
    "    return {\"qa_0\": {\"question\": state.qa_0.question, \"answer\": \"ë‹µë³€0\"}}\n",
    "\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    # state.qa_0, qa_1, qa_2ì— ë‹´ê¸´ ì§ˆë¬¸/ë‹µë³€ì„ ëª¨ì•„ì„œ ìµœì¢… summary, content ìƒì„±\n",
    "    content = [state.qa_0]\n",
    "    summary = \"ìš”ì•½ ìƒì„± ì™„ë£Œ\"\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3570c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/IPython/core/formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langgraph/pregel/__init__.py:634\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    633\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    635\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    287\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    288\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    289\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    290\u001b[39m         )\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:462\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m             msg = (\n\u001b[32m    459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    465\u001b[39m msg = (\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x7ff53e3d71d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7327878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f17a5eb6450>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ì •ì˜ ë° êµ¬ì„±\n",
    "from langgraph.graph import END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# ë…¸ë“œ ì„ ì–¸\n",
    "workflow.add_node(\"retriever\", RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "\n",
    "# ì§ˆë¬¸ ìƒì„± ë…¸ë“œ\n",
    "workflow.add_node(\"question0\", RunnableLambda(question0_node).with_config({\"run_name\": \"question0\"}))\n",
    "workflow.add_node(\"question1\", RunnableLambda(question1_node).with_config({\"run_name\": \"question1\"}))\n",
    "workflow.add_node(\"question2\", RunnableLambda(question2_node).with_config({\"run_name\": \"question2\"}))\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "workflow.add_node(\"answer0\", RunnableLambda(answer0_node).with_config({\"run_name\": \"answer0\"}))\n",
    "workflow.add_node(\"answer1\", RunnableLambda(answer1_node).with_config({\"run_name\": \"answer1\"}))\n",
    "workflow.add_node(\"answer2\", RunnableLambda(answer2_node).with_config({\"run_name\": \"answer2\"}))\n",
    "\n",
    "# fallback + summary\n",
    "# workflow.add_node(\"fallback_generate\", RunnableLambda(fallback_generate_node).with_config({\"run_name\": \"fallback_generate\"}))\n",
    "workflow.add_node(\"summary_node\", RunnableLambda(summary_node).with_config({\"run_name\": \"summary_node\"}))\n",
    "\n",
    "# ì¡°ê±´ë¶€ ë¶„ê¸° ì„¤ì •\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generate_seq\" if state.similarity_score >= 0.5 else \"fallback_generate\"\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retriever\",\n",
    "#     route_by_similarity,\n",
    "#     {\n",
    "#         \"generate_seq\": [\"question0\", \"question1\", \"question2\"],\n",
    "#         \"fallback_generate\": \"fallback_generate\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question0\")\n",
    "workflow.add_edge(\"retriever\", \"question1\")\n",
    "workflow.add_edge(\"retriever\", \"question2\")\n",
    "\n",
    "\n",
    "# ê° ì§ˆë¬¸ ìƒì„± â†’ ë‹µë³€ ìƒì„± ì—°ê²°\n",
    "workflow.add_edge(\"question0\", \"answer0\")\n",
    "workflow.add_edge(\"question1\", \"answer1\")\n",
    "workflow.add_edge(\"question2\", \"answer2\")\n",
    "\n",
    "# ê° ë‹µë³€ ë…¸ë“œ â†’ summary\n",
    "workflow.add_edge(\"answer0\", \"summary_node\")\n",
    "workflow.add_edge(\"answer1\", \"summary_node\")\n",
    "workflow.add_edge(\"answer2\", \"summary_node\")\n",
    "\n",
    "# fallbackë„ summaryë¡œ\n",
    "# workflow.add_edge(\"fallback_generate\", \"summary_node\")\n",
    "\n",
    "# ì‹œì‘, ì¢…ë£Œ ì§€ì  ì„¤ì •\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "workflow.set_finish_point(\"summary_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc49f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f30595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-08 08:49:23 [async_llm.py:228] Added request 04e11ffa-2ba6-4d30-ad82-5622e8a42637.\n"
     ]
    }
   ],
   "source": [
    "# # ê·¸ë˜í”„ ì •ì˜ ë° êµ¬ì„±\n",
    "# from langgraph.graph import END\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# workflow = StateGraph(QAState)\n",
    "\n",
    "# # ë…¸ë“œ ì„ ì–¸\n",
    "# workflow.add_node(\"retriever\", \n",
    "#     RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "# workflow.add_node(\"generation0\", \n",
    "#     RunnableLambda(generation0_node).with_config({\"run_name\": \"generation0\"}))\n",
    "# workflow.add_node(\"generation1\", \n",
    "#     RunnableLambda(generation1_node).with_config({\"run_name\": \"generation1\"}))\n",
    "# workflow.add_node(\"generation2\", \n",
    "#     RunnableLambda(generation2_node).with_config({\"run_name\": \"generation2\"}))\n",
    "# workflow.add_node(\"fallback_generate\", \n",
    "#     RunnableLambda(fallback_generate_node).with_config({\"run_name\": \"fallback_generate\"}))\n",
    "# workflow.add_node(\"summary_node\", \n",
    "#     RunnableLambda(summary_node).with_config({\"run_name\": \"summary_node\"}))\n",
    "\n",
    "# # ì‹œì‘ ì§€ì  ì„¤ì •\n",
    "# workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "# # ì¡°ê±´ë¶€ ë¶„ê¸°\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generation\" if state.similarity_score >= 0.50 else \"fallback_generate\"\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retriever\",\n",
    "#     route_by_similarity,\n",
    "#     {\n",
    "#         \"generation\": [\"generation0\", \"generation1\", \"generation2\"],\n",
    "#         \"fallback_generate\": \"fallback_generate\"\n",
    "#     })\n",
    "\n",
    "# # ë³‘ë ¬ ë¶„ê¸°: generation / fallback â†’ summary\n",
    "# workflow.add_edge(\"generation0\", \"summary_node\")\n",
    "# workflow.add_edge(\"generation1\", \"summary_node\")\n",
    "# workflow.add_edge(\"generation2\", \"summary_node\")\n",
    "# workflow.add_edge(\"fallback_generate\", \"summary_node\")\n",
    "\n",
    "# # ì¢…ë£Œ ì§€ì  ì„¤ì •\n",
    "# workflow.set_finish_point(\"summary_node\")\n",
    "\n",
    "# graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì´ê±´ ì…ë ¥ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤: LangGraph ì—°ë™ í…ŒìŠ¤íŠ¸'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langsmith import traceable\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# @traceable(name=\"generate-question\")\n",
    "# def generate_question(input_text: str) -> str:\n",
    "#     return f\"ì´ê±´ ì…ë ¥ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤: {input_text}\"\n",
    "\n",
    "# generate_question(\"LangGraph ì—°ë™ í…ŒìŠ¤íŠ¸\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
