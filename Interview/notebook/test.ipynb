{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a3115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri011228/ai2-server/deeplearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:34:36 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:34:38,199\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:34:47 [config.py:689] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-14 00:34:47 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-14 00:34:49 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/home/yuri011228/ai2-server/models/mistral-7b', speculative_config=None, tokenizer='/home/yuri011228/ai2-server/models/mistral-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/yuri011228/ai2-server/models/mistral-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-14 00:34:50 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1d9bf86f90>\n",
      "INFO 05-14 00:34:50 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-14 00:34:50 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-14 00:34:50 [gpu_model_runner.py:1276] Starting to load model /home/yuri011228/ai2-server/models/mistral-7b...\n",
      "WARNING 05-14 00:34:50 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.30s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:02<00:01,  1.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.45s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:34:55 [loader.py:458] Loading weights took 4.65 seconds\n",
      "INFO 05-14 00:34:55 [gpu_model_runner.py:1291] Model loading took 13.4967 GiB and 4.848054 seconds\n",
      "INFO 05-14 00:35:07 [backends.py:416] Using cache directory: /home/yuri011228/.cache/vllm/torch_compile_cache/8f3f217165/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-14 00:35:07 [backends.py:426] Dynamo bytecode transform time: 11.19 s\n",
      "INFO 05-14 00:35:08 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 05-14 00:35:16 [monitor.py:33] torch.compile takes 11.19 s in total\n",
      "INFO 05-14 00:35:19 [kv_cache_utils.py:634] GPU KV cache size: 49,296 tokens\n",
      "INFO 05-14 00:35:19 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 12.04x\n",
      "INFO 05-14 00:35:54 [gpu_model_runner.py:1626] Graph capturing finished in 35 secs, took 0.51 GiB\n",
      "INFO 05-14 00:35:54 [core.py:163] init engine (profile, create kv cache, warmup model) took 58.21 seconds\n",
      "INFO 05-14 00:35:54 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "base_llm = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=base_llm,\n",
    "    tensor_parallel_size=1, # GPU Í∞úÏàò\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_num_seqs = 100, # ÎèôÏãúÏóê Î∞õÏùÑ Ïàò ÏûàÎäî ÏöîÏ≤≠ Í∞úÏàò\n",
    "    max_model_len=4096, # input + output ÌÜ†ÌÅ∞ Í∏∏Ïù¥\n",
    "    max_num_batched_tokens=8192) # Ìïú batch Îãπ ÌÜ†Í∑º Í∏∏Ïù¥ \n",
    "\n",
    "llm = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ÌôïÏù∏Ïö©\n",
    "# os.environ[\"LANGSMITH_TRACING\"]  \n",
    "# print(os.getenv(\"QDRANT_HOST\"))\n",
    "# print(os.getenv(\"QDRANT_PORT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = {\n",
    "  \"email\": \"ConconDev\",\n",
    "  \"date\": \"2024-09-06\",\n",
    "  \"level\": 1,\n",
    "  \"title\": \"ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§ Í∞úÏÑ†: SseEmitter ÌôúÏö© Î∞è ÏÇ¨Ïö©ÏûêÎ≥Ñ Emitter Í¥ÄÎ¶¨\",\n",
    "  \"keywords\": [\n",
    "    \"SSE\",\n",
    "    \"SseEmitter\",\n",
    "    \"ÏïåÎûå Íµ¨ÎèÖ\",\n",
    "    \"ÏÇ¨Ïö©ÏûêÎ≥Ñ Í¥ÄÎ¶¨\"\n",
    "  ],\n",
    "  \"til\": \"# ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§ Í∞úÏÑ†: SseEmitter ÌôúÏö© Î∞è ÏÇ¨Ïö©ÏûêÎ≥Ñ Emitter Í¥ÄÎ¶¨\\n\\n## 1. Ïò§Îäò Î∞∞Ïö¥ ÎÇ¥Ïö©\\n\\nÏò§Îäò Ï†ÄÎäî ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`Î•º ÌôúÏö©ÌïòÍ≥†, Í∞Å ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Ï†ÄÏû•ÌïòÍ≥† Í¥ÄÎ¶¨ÌïòÎäî Í∏∞Îä•ÏùÑ Íµ¨ÌòÑÌñàÏäµÎãàÎã§.  `SseEmitter`Îäî ÏÑúÎ≤ÑÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞Î•º Ïä§Ìä∏Î¶¨Î∞çÌïòÎäî Îç∞ Ïú†Ïö©Ìïú APIÏûÖÎãàÎã§.\\n\\n## 2. Í∞úÎÖê Ï†ïÎ¶¨\\n\\n*   **SSE (Server-Sent Events):** ÏÑúÎ≤ÑÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Îã®Î∞©Ìñ• ÌÜµÏã†ÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïòÎäî Ïõπ Í∏∞Ïà†ÏûÖÎãàÎã§. ÏÑúÎ≤ÑÍ∞Ä ÏÉàÎ°úÏö¥ Ïù¥Î≤§Ìä∏ Î∞úÏÉù Ïãú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÍ≤å ÏûêÎèôÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Î•º Ï†ÑÏÜ°Ìï©ÎãàÎã§.\\n*   **SseEmitter:** SSE ÌÜµÏã†ÏùÑ ÏúÑÌïú Í∞ùÏ≤¥ÏûÖÎãàÎã§.  Îç∞Ïù¥ÌÑ∞Î•º Î∞úÌñâÌïòÍ±∞ÎÇò, Ïó∞Í≤∞ÏùÑ Ï¢ÖÎ£åÌïòÍ±∞ÎÇò, Ïò§Î•òÎ•º Ï≤òÎ¶¨ÌïòÎäî Îì±Ïùò Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\\n*   **ConcurrentHashMap:** Ïó¨Îü¨ Ïä§Î†àÎìúÏóêÏÑú ÎèôÏãúÏóê Ï†ëÍ∑ºÌï¥ÎèÑ ÏïàÏ†ÑÌïòÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§.  Ïó¨Í∏∞ÏÑúÎäî ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú ÌïòÍ≥† `SseEmitter`Î•º Í∞íÏúºÎ°ú Ï†ÄÏû•ÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÏóàÏäµÎãàÎã§.\\n\\n## 3. Ìï¥Îãπ Í∞úÎÖêÏù¥ ÌïÑÏöîÌïú Ïù¥Ïú†\\n\\nÍ∏∞Ï°¥ ÏïåÎûå Íµ¨ÎèÖ Î∞©ÏãùÏùÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÏùÑ Î≥¥ÎÇ¥Îäî Î∞©ÏãùÏúºÎ°ú, ÏÑúÎ≤Ñ Î∂ÄÌïòÍ∞Ä Ïã¨ÌïòÍ≥† Ìö®Ïú®ÏÑ±Ïù¥ Îñ®Ïñ¥Ï°åÏäµÎãàÎã§. `SseEmitter`Î•º ÏÇ¨Ïö©ÌïòÎ©¥ ÏÑúÎ≤ÑÎäî ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌñàÏùÑ ÎïåÎßå ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏÜ°ÌïòÎØÄÎ°ú, ÏÑúÎ≤Ñ ÏûêÏõêÏùÑ Ï†àÏïΩÌïòÍ≥† Ïã§ÏãúÍ∞ÑÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç®, ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.\\n\\n## 4. Í∞úÎÖêÏùÑ ÌôúÏö©ÌïòÎäî Î∞©Î≤ï\\n\\n1.  `SseEmitter` Í∞ùÏ≤¥Î•º ÏÉùÏÑ±ÌïòÍ≥† Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.\\n2.  ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú, `SseEmitter` Í∞ùÏ≤¥Î•º Í∞íÏúºÎ°ú `ConcurrentHashMap`Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§.\\n3.  ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÏÑú SSE Ïó∞Í≤∞ÏùÑ ÏÑ§Ï†ïÌïòÍ≥†, ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏã†Ìï©ÎãàÎã§.\\n4.  ÏÇ¨Ïö©ÏûêÍ∞Ä ÏïåÎûå Íµ¨ÎèÖ/Ï∑®ÏÜå ÏöîÏ≤≠ÏùÑ ÌïòÎ©¥, Ìï¥Îãπ ÏÇ¨Ïö©ÏûêÏùò `SseEmitter`Î•º ÏóÖÎç∞Ïù¥Ìä∏ÌïòÍ±∞ÎÇò ÏÇ≠Ï†úÌï©ÎãàÎã§.\\n\\n## 5. Î¨∏Ï†ú Ìï¥Í≤∞ Í≥ºÏ†ï\\n\\n*   Ï≤òÏùåÏóêÎäî `SseEmitter`Ïùò ÏÉùÎ™ÖÏ£ºÍ∏∞Î•º Ï†úÎåÄÎ°ú Í¥ÄÎ¶¨ÌïòÏßÄ Î™ªÌïòÏó¨ Î©îÎ™®Î¶¨ ÎàÑÏàòÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.  `SseEmitter` Í∞ùÏ≤¥Ïùò `close()` Î©îÏÑúÎìúÎ•º Ìò∏Ï∂úÌïòÏó¨ Î¶¨ÏÜåÏä§ ÎàÑÏàòÎ•º Î∞©ÏßÄÌñàÏäµÎãàÎã§.\\n*   ÏÇ¨Ïö©Ïûê ID Ï§ëÎ≥µ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú ÏÇ¨Ïö©ÌïòÎäî `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§.\\n*   ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÏÑú SSE Ïó∞Í≤∞ÏùÑ ÏïàÏ†ïÏ†ÅÏúºÎ°ú Ïú†ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ ÏóêÎü¨ Ìï∏Îì§ÎßÅ Î°úÏßÅÏùÑ Ï∂îÍ∞ÄÌñàÏäµÎãàÎã§.\\n\\n## 6. ÌïòÎ£® ÌöåÍ≥†\\n\\nÏò§ÎäòÏùÄ ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò ÏÑ±Îä• Í∞úÏÑ†ÏùÑ ÏúÑÌï¥ Ï§ëÏöîÌïú Í∏∞Ïà†Ïù∏ `SseEmitter`Î•º ÏùµÌûàÍ≥† Ï†ÅÏö©ÌïòÎäî ÏãúÍ∞ÑÏùÑ Í∞ÄÏ°åÏäµÎãàÎã§.  `SseEmitter`Ïùò ÎèôÏûë ÏõêÎ¶¨Î•º Ïù¥Ìï¥ÌïòÍ≥†, Ïã§Ï†ú ÏÑúÎπÑÏä§Ïóê Ï†ÅÏö©ÌïòÎ©¥ÏÑú ÎßéÏùÄ Ïñ¥Î†§ÏõÄÏùÑ Í≤™ÏóàÏßÄÎßå, Í≤∞Íµ≠ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Íµ¨ÌòÑÌï† Ïàò ÏûàÏóàÏäµÎãàÎã§. ÏïûÏúºÎ°úÎäî `SseEmitter`Î•º Îçî ÍπäÏù¥ Ïù¥Ìï¥ÌïòÍ≥†, Îã§ÏñëÌïú ÏùëÏö© Î∂ÑÏïºÏóê Ï†ÅÏö©Ìï¥Î≥¥Í≥† Ïã∂ÏäµÎãàÎã§.\\n\\n## 7. Ï†ÑÏ≤¥Ï†ÅÏúºÎ°ú Í∞úÏ°∞Ïãù Î¨∏Ïû• Íµ¨ÏÑ±\\n\\n*   **Î™©Ìëú:** ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò Ïã§ÏãúÍ∞ÑÏÑ± Î∞è Ìö®Ïú®ÏÑ± Ìñ•ÏÉÅ\\n*   **ÌïµÏã¨ Í∏∞Ïà†:** SSE, SseEmitter, ConcurrentHashMap\\n*   **Íµ¨ÌòÑ Îã®Í≥Ñ:**\\n    *   `SseEmitter` Í∞ùÏ≤¥ ÏÉùÏÑ± Î∞è Ï¥àÍ∏∞Ìôî\\n    *   ÏÇ¨Ïö©ÏûêÎ≥Ñ `SseEmitter` Ï†ÄÏû• Î∞è Í¥ÄÎ¶¨ (`ConcurrentHashMap`) \\n    *   SSE Ïó∞Í≤∞ ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ ÏàòÏã†\\n    *   ÏïåÎûå Íµ¨ÎèÖ/Ï∑®ÏÜå ÏöîÏ≤≠ Ï≤òÎ¶¨\\n    *   Î©îÎ™®Î¶¨ ÎàÑÏàò Î∞©ÏßÄ Î∞è ÏóêÎü¨ Ìï∏Îì§ÎßÅ\\n\\n\"\n",
    "}\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "You are a technical interviewer AI.\n",
    "\n",
    "Your task is to generate exactly one interview question written in Korean, based on the following inputs:\n",
    "\n",
    "- Level: {level}\n",
    "- User TIL: {til}\n",
    "- Reference documents: {retrieved}\n",
    "\n",
    "## Output Instructions (strict):\n",
    "- Your response must be a **single complete sentence** in **Korean**.\n",
    "- The sentence must be a **clear interview-style question**, using natural question forms such as:\n",
    "  ‚Äú~ÏûÖÎãàÍπå?‚Äù, ‚Äú~ÏûàÎÇòÏöî?‚Äù, ‚Äú~ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî‚Äù, ‚Äú~Ïñ¥ÎñªÍ≤å ÎêòÎÇòÏöî?‚Äù, ‚Äú~Ïñ¥ÎñªÍ≤å ÏÉùÍ∞ÅÌïòÏãúÎÇòÏöî?‚Äù etc.\n",
    "- It must **not** be a declarative or answer-style sentence (e.g., ending with ‚Äú~ÏûÖÎãàÎã§‚Äù, ‚Äú~Ìï©ÎãàÎã§‚Äù ‚ùå)\n",
    "\n",
    "- ‚ö†Ô∏è Do NOT include any of the following:\n",
    "  - English words or explanations\n",
    "  - Headings, notes, or comments\n",
    "  - Labels such as ‚ÄúQuestion:‚Äù, ‚ÄúAnswer:‚Äù, ‚ÄúNote:‚Äù, or anything similar\n",
    "  - Markdown symbols (e.g., **, ``, ‚Üí, #, ##)\n",
    "  - Emojis, quotation marks, parentheses, or line breaks\n",
    "\n",
    "- Only write the Korean question sentence. Nothing else.\n",
    "\n",
    "## Depth Control:\n",
    "- Level 1: Ask about deep technical understanding and implementation logic\n",
    "- Level 2: Ask about conceptual understanding\n",
    "- Level 3: Ask about basic theoretical concepts\n",
    "\n",
    "Respond with only one clean Korean question sentence. No explanations, no formatting, no extra text.\n",
    "\n",
    "question:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "You are an AI assistant that answers a technical interview question based on the user's learning record.\n",
    "\n",
    "Here is the input:\n",
    "- Question: {question}\n",
    "- User TIL: {til}\n",
    "- Level: {level}\n",
    "- Reference documents: {context}\n",
    "\n",
    "Do not repeat the question.  \n",
    "Generate **only one answer**, in **Korean**, based on the above information.  \n",
    "Keep your answer **concise**, **clear**, and **free of unnecessary symbols** or decorations.\n",
    "\n",
    "Just provide the answer in plain Korean. No introduction or explanation is needed.\n",
    "\n",
    "answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = \"\"\"\n",
    "You are an AI assistant that summarizes a technical interview question and its answer into a short, meaningful Korean title.\n",
    "\n",
    "Your goal is to create a clear and specific title that would fit well in a developer document or a technical spec.\n",
    "\n",
    "Requirements:\n",
    "- The title must be written in **Korean**\n",
    "- The title must be **15 characters or fewer**\n",
    "- Do NOT include any quotation marks, punctuation, or extra lines\n",
    "- Write only the final title\n",
    "\n",
    "Example:\n",
    "Q: REST APIÎûÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?  \n",
    "A: REST APIÎäî HTTP ÌîÑÎ°úÌÜ†ÏΩúÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏûêÏõêÏùÑ URIÎ°ú ÌëúÌòÑÌïòÍ≥†, CRUDÎ•º HTTP Î©îÏÑúÎìúÎ°ú ÏàòÌñâÌïòÎäî ÏïÑÌÇ§ÌÖçÏ≤òÏûÖÎãàÎã§.  \n",
    "title: REST API Í∞úÎÖê Î∞è Íµ¨ÏÑ± ÏöîÏÜå\n",
    "\n",
    "Now summarize the following Q&A in the same way.\n",
    "\n",
    "{qacombined}\n",
    "\n",
    "title:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a71449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class ContentState(BaseModel):\n",
    "    question: str\n",
    "    answer: str \n",
    "\n",
    "class QAState(BaseModel):\n",
    "    email: str\n",
    "    date: str\n",
    "    level: int\n",
    "    title: str\n",
    "    keywords: List[str]\n",
    "    til: str\n",
    "\n",
    "    retrieved_texts: Optional[List[str]] = None\n",
    "    similarity_score: Optional[float] = None\n",
    "\n",
    "    question0: Optional[str] = None\n",
    "    question1: Optional[str] = None\n",
    "    question2: Optional[str] = None\n",
    "\n",
    "    question: Optional[str] = None\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "    content0: Optional[ContentState] = None\n",
    "    content1: Optional[ContentState] = None\n",
    "    content2: Optional[ContentState] = None\n",
    "\n",
    "    #output\n",
    "    content: Optional[List[ContentState]] = None\n",
    "    summary: Optional[str] = None\n",
    "\n",
    "\n",
    "qa_input = QAState(\n",
    "    email=dummy['email'],\n",
    "    date=dummy['date'],\n",
    "    level=dummy['level'],\n",
    "    title=dummy['title'],\n",
    "    keywords=dummy['keywords'],\n",
    "    til=dummy['til']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee0027",
   "metadata": {},
   "source": [
    "### ÌÅ¥ÎûòÏä§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d032c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "import re\n",
    "\n",
    "class QAFlow:\n",
    "    def __init__(self, llm, qdrant, prompt1, prompt2, prompt3, max_nodes=3):\n",
    "        self.llm = llm\n",
    "        self.qdrant = qdrant\n",
    "        self.prompt1 = prompt1\n",
    "        self.prompt2 = prompt2\n",
    "        self.prompt3 = prompt3\n",
    "        self.max_nodes = max_nodes\n",
    "        self.embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "    def embed_text(self, text: str) -> list[float]:\n",
    "        return self.embedding_model.encode(text).tolist()\n",
    "\n",
    "    async def retriever_node(self, state: QAState) -> dict:\n",
    "        query = state.title + \" \" + \" \".join(state.keywords)\n",
    "        query_vector = self.embed_text(query)\n",
    "\n",
    "        collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "        best_score = 0.0\n",
    "        retrieved_texts = []\n",
    "\n",
    "        for col in collection_names:\n",
    "            results = self.qdrant.search(\n",
    "                collection_name=col,\n",
    "                query_vector=query_vector,\n",
    "                limit=3,\n",
    "                with_payload=True\n",
    "            )\n",
    "            if results and results[0].score > best_score:\n",
    "                best_score = results[0].score\n",
    "                retrieved_texts = [r.payload[\"text\"] for r in results if \"text\" in r.payload]\n",
    "\n",
    "        return {\n",
    "            \"similarity_score\": best_score,\n",
    "            \"retrieved_texts\": retrieved_texts\n",
    "        }\n",
    "    \n",
    "    # ÌõÑÏ≤òÎ¶¨ Ï∂îÍ∞Ä \n",
    "    def clean_korean_question(self, text: str) -> str:\n",
    "\n",
    "        text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "\n",
    "        # ÎùºÎ≤® Î∞è ÎßàÌÅ¨Îã§Ïö¥ Ï†úÍ±∞\n",
    "        text = re.sub(r'\\*\\*?(Question|Answer|Note|Level).*?\\*\\*?', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'(Question|Answer|Level)\\s*[:Ôºö]*', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'^#+\\s*', '', text)\n",
    "\n",
    "        # Î¨∏Ïû• Îß® Ïïû ÌïòÏù¥Ìîà/Î≤àÌò∏ Ï†úÍ±∞\n",
    "        text = re.sub(r'^[-‚Ä¢\\s]+\\d*\\s*', '', text)\n",
    "\n",
    "        # Í¥ÑÌò∏ level (ÎùÑÏñ¥Ïì∞Í∏∞ Ìè¨Ìï®) Ï†úÍ±∞\n",
    "        text = re.sub(r'\\(\\s*\\d+\\s*\\)', '', text)\n",
    "\n",
    "        # Í∏∞ÌÉÄ ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞\n",
    "        text = text.replace(\"`\", \"\").replace(\"‚Äú\", \"\").replace(\"‚Äù\", \"\")\n",
    "        text = text.replace(\"üëâ\", \"\").replace(\"‚Üí\", \"\").strip()\n",
    "        text = text.strip().strip('\"‚Äú‚Äù')\n",
    "\n",
    "        # Ï§Ñ Îã®ÏúÑÎ°ú ÎÇòÎàÑÍ∏∞\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "        question_endings = [\"?\", \"Ïöî.\", \"ÏäµÎãàÍπå\", \"ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\", \"Ïñ¥ÎñªÍ≤å\", \"Î¨¥Ïóá\", \n",
    "                            \"ÏÑ§Î™ÖÌïòÏãúÏò§\", \"Íµ¨ÌòÑÌïòÏãúÏò§\", \"ÏïåÎ†§Ï£ºÏÑ∏Ïöî\", \"Ïñ¥Îñ§Í∞ÄÏöî\", \"Ïôú Í∑∏Îü∞Í∞ÄÏöî\"]\n",
    "        \n",
    "        # ÏôÑÍ≤∞Îêú ÏßàÎ¨∏Ìòï Î¨∏Ïû•Îßå ÌÉêÏÉâ\n",
    "        for line in lines:\n",
    "            if any(ending in line for ending in question_endings):\n",
    "                return line\n",
    "\n",
    "        return lines[0] if lines else \"\"\n",
    "\n",
    "\n",
    "    def generate_question_node(self, node_id: int):\n",
    "        async def question_node(state: QAState) -> dict:\n",
    "            retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "            \n",
    "            prompt1 = self.prompt1.format(\n",
    "                til=state.til,\n",
    "                level=state.level,\n",
    "                retrieved=retrieved\n",
    "            )\n",
    "\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                max_tokens=128,\n",
    "                stop_token_ids=[2]\n",
    "            )\n",
    "\n",
    "            request_id = str(uuid4())\n",
    "            final_text = \"\"\n",
    "\n",
    "            async for output in self.llm.generate(\n",
    "                prompt=prompt1,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id\n",
    "            ):\n",
    "                final_text = output.outputs[0].text.strip()\n",
    "\n",
    "            cleaned_question = self.clean_korean_question(final_text)\n",
    "\n",
    "            # print(node_id, final_text)\n",
    "            # print(node_id, cleaned_question)\n",
    "\n",
    "            return {f\"question{node_id}\": cleaned_question}\n",
    "\n",
    "        return question_node\n",
    "\n",
    "    def generate_answer_node(self, node_id: int):\n",
    "        async def answer_node(state: QAState) -> dict:\n",
    "            question = getattr(state, f\"question{node_id}\", None)\n",
    "            if not question:\n",
    "                raise ValueError(f\"ÏßàÎ¨∏ {node_id}Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "            context = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "            prompt2 = self.prompt2.format(\n",
    "                question=question,\n",
    "                til=state.til,\n",
    "                level=state.level,\n",
    "                context=context\n",
    "            )\n",
    "\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                max_tokens=256,\n",
    "                stop_token_ids=[2]\n",
    "            )\n",
    "\n",
    "            request_id = str(uuid4())\n",
    "            final_text = \"\"\n",
    "\n",
    "            async for output in self.llm.generate(\n",
    "                prompt=prompt2,\n",
    "                sampling_params=sampling_params,\n",
    "                request_id=request_id\n",
    "            ):\n",
    "                final_text = output.outputs[0].text.strip()\n",
    "\n",
    "            return {\n",
    "                #\"content\": [ContentState(question=question, answer=final_text)]\n",
    "                f\"content{node_id}\": ContentState(\n",
    "                    question=question,\n",
    "                    answer=final_text)\n",
    "            }\n",
    "\n",
    "        return answer_node\n",
    "\n",
    "    async def summary_node(self, state: QAState) -> dict:\n",
    "        merged = []\n",
    "        for i in range(3):\n",
    "            item = getattr(state, f\"content{i}\", None)\n",
    "            if item:\n",
    "                merged.append(item)\n",
    "\n",
    "        qacombined = \"\\n\".join(\n",
    "            f\"Q: {item.question}\\nA: {item.answer}\" for item in merged\n",
    "        )\n",
    "\n",
    "        prompt3 = self.prompt3.format(\n",
    "            qacombined = qacombined\n",
    "        )\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.3,\n",
    "            max_tokens=32,\n",
    "            stop_token_ids=[2]\n",
    "        )\n",
    "\n",
    "        request_id = str(uuid4())\n",
    "        final_text = \"\"\n",
    "\n",
    "        async for output in self.llm.generate(\n",
    "            prompt=prompt3,\n",
    "            sampling_params=sampling_params,\n",
    "            request_id=request_id\n",
    "        ):\n",
    "            final_text = output.outputs[0].text.strip()\n",
    "\n",
    "        return {\n",
    "            \"summary\": final_text,\n",
    "            \"content\": merged\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        workflow = StateGraph(QAState)\n",
    "        workflow.set_entry_point(\"retriever\")\n",
    "        workflow.add_node(\"retriever\", self.retriever_node)\n",
    "\n",
    "        for i in range(self.max_nodes):\n",
    "            workflow.add_node(f\"que{i}\", self.generate_question_node(i))\n",
    "            workflow.add_node(f\"ans{i}\", self.generate_answer_node(i))\n",
    "\n",
    "            workflow.add_edge(\"retriever\", f\"que{i}\")\n",
    "            workflow.add_edge(f\"que{i}\", f\"ans{i}\")\n",
    "            workflow.add_edge(f\"ans{i}\", \"summary_generate\")\n",
    "\n",
    "        workflow.add_node(\"summary_generate\", self.summary_node)\n",
    "        workflow.set_finish_point(\"summary_generate\")\n",
    "\n",
    "        return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a844486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:36:36 [async_llm.py:228] Added request 1da07a71-2f2a-40c5-a209-a65cdbc79c1a.\n",
      "INFO 05-14 00:36:36 [async_llm.py:228] Added request 8b6871dc-2652-4781-ab3d-8a7c8b961a11.\n",
      "INFO 05-14 00:36:36 [async_llm.py:228] Added request dc997c6d-a70b-41ce-a233-2b744d6ff28f.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13410/2998371373.py:29: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 00:36:43 [async_llm.py:228] Added request 20c84df0-f8dd-430d-be02-63f208e9807d.\n",
      "INFO 05-14 00:36:43 [async_llm.py:228] Added request 47b8886c-186c-4dd4-af8d-5cf525aae9e0.\n",
      "INFO 05-14 00:36:43 [async_llm.py:228] Added request 259541c8-e817-48fa-ba6a-3e18e360dca5.\n",
      "INFO 05-14 00:37:01 [async_llm.py:228] Added request fa34bed2-9509-43b8-abe9-b805656afcca.\n",
      "SSE ÏïåÎûå ÏÑúÎπÑÏä§: ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitter Í¥ÄÎ¶¨Ïö© ConcurrentHashMap\n",
      "\n",
      "title:\n",
      "\n",
      "S\n",
      "Ïñ¥Îñ§ Î∞©ÏãùÏúºÎ°ú SseEmitter Í∞ùÏ≤¥Î•º ÏÉùÏÑ±ÌïòÍ≥† ÏÇ¨Ïö©ÏûêÎ≥Ñ Ï†ÄÏû• Î∞è Í¥ÄÎ¶¨ÌïòÎäî ConcurrentHashMapÏùÑ Íµ¨ÏÑ±ÌïòÏó¨, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÎäîÎç∞ ÎèÑÏõÄÏù¥ ÎêòÎäîÏßÄ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî?\n",
      "SSE(ÏÑúÎ≤Ñ Ï†ÑÏÜ° Ïù¥Î≤§Ìä∏)Î•º ÌôúÏö©ÌïòÏó¨, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitter Í∞ùÏ≤¥Î•º ÏÉùÏÑ±ÌïòÍ≥† ConcurrentHashMapÏùÑ Ïù¥Ïö©ÌïòÏó¨ Í¥ÄÎ¶¨ÌïòÎäî Î∞©Î≤ïÏûÖÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥, Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Ïä§Ìä∏Î¶ºÏùÑ ÏßÄÏÜçÏ†ÅÏúºÎ°ú Ï†ÑÏÜ°ÌïòÎ©¥ÏÑúÎèÑ Î©îÎ™®Î¶¨ ÎàÑÏàò Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ≥†, ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóêÍ≤åÎßå ÏïåÎûåÏùÑ Ï†ÑÏÜ°ÌïòÎäî Í≤ÉÏù¥ Í∞ÄÎä•Ìï¥ÏßëÎãàÎã§. ÏõπÎ∏åÎùºÏö∞Ï†ÄÎäî ÏµúÏã† Î≤ÑÏ†ÑÎ∂ÄÌÑ∞ Î™®Îì† Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú SSEÎ•º ÏßÄÏõêÌïòÍ≥† ÏûàÏúºÎ©∞, Ïù¥Î•º ÌôúÏö©ÌïòÏó¨ ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò Ïã§ÏãúÍ∞ÑÏÑ±Í≥º Ìö®Ïú®ÏÑ±ÏùÑ Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitterÎ•º Í¥ÄÎ¶¨ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.\n",
      "SSE(Server-Sent Events)Îäî ÏÇ¨Ïö©ÏûêÍ∞Ä HTTP Ïó∞Í≤∞ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏Î•º Î∞õÎäî ÏÑúÎ≤Ñ Ìë∏Ïãú Í∏∞Ïà†ÏûÖÎãàÎã§. `SseEmitter`Îäî SSEÎ•º Íµ¨ÌòÑÌïòÎäî Í∞ùÏ≤¥ÏûÖÎãàÎã§. Í∞Å ÏÇ¨Ïö©ÏûêÎäî Í≥†Ïú†Ïùò `SseEmitter`Î•º Í∞ÄÏßÄÍ≥† ÏûàÏñ¥ Ïã§ÏãúÍ∞ÑÏúºÎ°ú Îç∞Ïù¥ÌÑ∞Î•º Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§. `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú, `SseEmitter`Î•º Í∞íÏúºÎ°ú Ï†ÄÏû•ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå ÏÑúÎπÑÏä§Î•º Íµ¨ÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitterÎ•º Í¥ÄÎ¶¨ÌïòÎäî ConcurrentHashMapÏóê ÎåÄÌï¥, Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Ïùò Ïû•Ï†êÍ≥º Îã®Ï†êÏùÄ Ïñ¥ÎñªÍ≤å  diff√©rences: Ôºü(ConcurrentHashMapÏùò ÏÇ¨Ïö©Ïûê Î≥Ñ SseEmitter Í¥ÄÎ¶¨Ïóê ÎåÄÌïú Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Ïùò Ïû•Ï†êÍ≥º Îã®Ï†êÏùÄ Ïñ¥ÎñªÍ≤å Îã§Î•¥ÎÇòÏöî?)\n",
      "ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitterÎ•º Í¥ÄÎ¶¨ÌïòÎäî ConcurrentHashMapÏùò Ïû•Ï†êÏùÄ Î©ÄÌã∞Ïä§Î†àÎìú ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÏïàÏ†ÑÌïòÍ≤å ÏÇ¨Ïö©ÏûêÏùò SseEmitter Í∞ùÏ≤¥Î•º Ï†ÄÏû•ÌïòÍ≥† Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÎã§Îäî Ï†êÏûÖÎãàÎã§. Ïó¨Í∏∞ÏÑú ConcurrentHashMapÎäî Ïó¨Îü¨ Ïä§Î†àÎìúÍ∞Ä ÎèôÏãúÏóê ÎèôÏùºÌïú Îç∞Ïù¥ÌÑ∞Î•º Ï†ëÍ∑ºÌïòÏó¨ÎèÑ ÏïàÏ†ÑÌïòÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§. ÎòêÌïú, ÏÇ¨Ïö©Ïûê IDÎ•º Í≥†Ïú†Ìïú ÌÇ§Î°ú ÏÇ¨Ïö©Ìï®ÏúºÎ°úÏç® ÏÇ¨Ïö©Ïûê Ï§ëÎ≥µ Î¨∏Ï†úÎ•º Ìï¥Í≤∞Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitterÎ•º Í¥ÄÎ¶¨ÌïòÎäî ConcurrentHashMapÏùò Îã®Ï†êÏùÄ ÎèôÍ∏∞Ìôî ÎπÑÏö©Ïù¥ ÎÜíÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ConcurrentHashMapÏùÄ ÏùºÎ∂Ä Í≤ΩÏö∞ÏóêÏÑú ÎèôÍ∏∞ÌôîÎ•º ÏÇ¨Ïö©Ìïò\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_flow = QAFlow(llm=llm, qdrant=qdrant, prompt1=prompt1, prompt2=prompt2, prompt3=prompt3, max_nodes=3)\n",
    "graph = qa_flow.build_graph()\n",
    "result = await graph.ainvoke(qa_input)\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(result[\"summary\"])\n",
    "\n",
    "for c in result[\"content\"]:\n",
    "    print(c.question)\n",
    "    print(c.answer)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d8147c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f953f6f6fd0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e5edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretriever(retriever)\n",
      "\tque0(que0)\n",
      "\tans0(ans0)\n",
      "\tque1(que1)\n",
      "\tans1(ans1)\n",
      "\tque2(que2)\n",
      "\tans2(ans2)\n",
      "\tsummary_generate(summary_generate)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retriever;\n",
      "\tans0 --> summary_generate;\n",
      "\tans1 --> summary_generate;\n",
      "\tans2 --> summary_generate;\n",
      "\tque0 --> ans0;\n",
      "\tque1 --> ans1;\n",
      "\tque2 --> ans2;\n",
      "\tretriever --> que0;\n",
      "\tretriever --> que1;\n",
      "\tretriever --> que2;\n",
      "\tsummary_generate --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_flow = QAFlow(llm=llm, qdrant=qdrant, prompt_template=prompt1, max_nodes=3)\n",
    "graph = qa_flow.build_graph()\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî ÏΩîÎìú\n",
    "print(graph.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5177459",
   "metadata": {},
   "source": [
    "### Ìï®Ïàò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e018ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity_score': 0.52782375, 'retrieved_texts': ['ÏÑúÎ≤Ñ Ï†ÑÏÜ° Ïù¥Î≤§Ìä∏(Server-sent events, SSE)Îäî ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä HTTP Ïó∞Í≤∞ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÏã†Ìï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî ÏÑúÎ≤Ñ Ìë∏Ïãú Í∏∞Ïà†Ïù¥Î©∞, Ï¥àÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞Ïù¥ ÏÑ§Ï†ïÎêú ÌõÑ ÏÑúÎ≤ÑÍ∞Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º Ìñ•Ìïú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏÜ°ÏùÑ ÏãúÏûëÌïòÎäî Î∞©Î≤ïÏùÑ ÏÑ§Î™ÖÌïúÎã§. Ïù¥Îäî ÏùºÎ∞òÏ†ÅÏúºÎ°ú Î∏åÎùºÏö∞Ï†Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Î©îÏãúÏßÄ ÏóÖÎç∞Ïù¥Ìä∏ ÎòêÎäî ÏßÄÏÜçÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ïä§Ìä∏Î¶ºÏùÑ Î≥¥ÎÇ¥Îäî Îç∞ ÏÇ¨Ïö©ÎêòÎ©∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ïù¥Î≤§Ìä∏ Ïä§Ìä∏Î¶ºÏùÑ ÏàòÏã†ÌïòÍ∏∞ ÏúÑÌï¥ ÌäπÏ†ï URLÏùÑ ÏöîÏ≤≠ÌïòÎäî EventSourceÎùºÎäî ÏûêÎ∞îÏä§ÌÅ¨Î¶ΩÌä∏ APIÎ•º ÌÜµÌï¥ Í∏∞Î≥∏ Î∏åÎùºÏö∞Ï†Ä Í∞Ñ Ïä§Ìä∏Î¶¨Î∞çÏùÑ Ìñ•ÏÉÅÏãúÌÇ§ÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÎã§. EventSource APIÎäî WHATWGÏóê ÏùòÌï¥ HTML Living StandardÏùò ÏùºÎ∂ÄÎ°ú ÌëúÏ§ÄÌôîÎêòÏóàÎã§. SSEÏùò ÎØ∏ÎîîÏñ¥ Ïú†ÌòïÏùÄ text/event-streamÏù¥Îã§. ÌååÏù¥Ïñ¥Ìè≠Ïä§ 6+, Íµ¨Í∏Ä ÌÅ¨Î°¨ 6+, Ïò§ÌéòÎùº 11.5+, ÏÇ¨ÌååÎ¶¨ 5+, ÎßàÏù¥ÌÅ¨Î°úÏÜåÌîÑÌä∏ Ïó£ÏßÄ 79+ Îì± Î™®Îì† ÏµúÏã† Î∏åÎùºÏö∞Ï†ÄÎäî ÏÑúÎ≤ÑÏóêÏÑú Ï†ÑÏÜ°ÎêòÎäî Ïù¥Î≤§Ìä∏Î•º ÏßÄÏõêÌïúÎã§.', 'Ìë∏Ïãú Í∏∞Î≤ï Í∏∞Ïà†ÏùÄ ÌíÄ Í∏∞Î≤ïÏúºÎ°ú Î™ÖÎ™ÖÎêú ÏõπÎ∏åÎùºÏö∞Ï†ÄÏôÄ ÎπÑÍµêÌï† Ïàò ÏûàÎã§. ÌòÑÏû¨ Ïù∏ÌÑ∞ÎÑ∑ÏóêÏÑú ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÎäî ÏõπÎ∏åÎùºÏö∞Ï†ÄÎäî ÏÇ¨Ïö©Ìï† ÎïåÏóê ÏÇ¨Ïö©ÏûêÍ∞Ä Ìï≠ÏÉÅ ÏûêÏã†Ïù¥ Í∞ñÍ≥†Ïûê ÌïòÎäî Ï†ïÎ≥¥Î•º ÏÜåÏú†Ìïú ÏÑúÎ≤ÑÏóêÍ≤å Ï†ïÎ≥¥Î•º ÏöîÏ≤≠ÌïòÎ©∞ ÏõπÎ∏åÎùºÏö∞Ï†ÄÏùò ÏÇ¨Ïö©Í≥º Î™©Ï†ÅÏßÄÏóê ÎåÄÌïú ÏµúÏ¢ÖÍ≤∞Ï†ï Ïó≠Ïãú ÏÇ¨Ïö©ÏûêÍ∞Ä Í≤∞Ï†ïÌï† Ïàò ÏûàÎã§. Î∞òÎ©¥ Ìë∏Ïãú Í∏∞Î≤ïÏùÄ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏùºÏùºÏù¥ ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏïÑÎèÑ ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏûêÎèôÏúºÎ°ú Îâ¥Ïä§ÎÇò ÏÇ¨Ïö©ÏûêÍ∞Ä ÏõêÌïòÎäî ÌäπÎ≥ÑÌïú Ï†ïÎ≥¥, ÏòàÎ•º Îì§Î©¥ Ï¶ùÍ∂åÏãúÏû•Ïùò Ï£ºÍ∏∞Ï†ÅÏù∏ Ï†ïÎ≥¥ Í∞ôÏùÄ Í≤ÉÏùÑ Ï†úÍ≥µÌïúÎã§. Ïù¥ Îëê Í∞ÄÏßÄ Í∏∞Î≤ïÏùò Ï∞®Ïù¥Ï†êÏùÄ Ï†ïÎ≥¥Ïùò ÌùêÎ¶ÑÏùÑ ÎàÑÍ∞Ä ÌÜµÏ†úÌïòÎäêÎÉê ÌïòÎäî Ï†êÏóê ÏûàÎã§Í≥† Ìï† Ïàò ÏûàÎã§. ÌíÄ Í∏∞Î≤ï ÌïòÏóêÏÑúÎäî ÏÇ¨Ïö©Ïûê (Ï¶â ÏÜåÎπÑÏûê)Îì§Ïù¥ Ï†ïÎ≥¥ Ï∑®Îìù Î∞è Ï†ïÎ≥¥Ïùò Ï†ëÏ¥âÏùÑ ÎßàÏùåÎåÄÎ°ú ÌÜµÏ†úÌï† Ïàò ÏûàÏúºÎÇò, Ìë∏Ïãú Í∏∞Î≤ï ÌïòÏóêÏÑúÎäî Ï†ïÎ≥¥Î•º Ï†ÑÎã¨ÌïòÎäî Ï™ΩÏóêÏÑú(Ï¶â Í¥ëÍ≥†Ï£º)Ï†ïÎ≥¥Ïùò ÌùêÎ¶ÑÏùÑ ÏßÅÏ†ë ÌÜµÏ†úÌï† Ïàò ÏûàÍ≤å ÎêúÎã§.', 'Ìë∏Ïãú Í∏∞Î≤ïÏùò Í∞ÄÏû• ÌÅ∞ Ïù¥Ï†êÏùÄ Ïó≠Ïãú Ï†ïÎ≥¥Ïùò ÎßûÏ∂§Ìôî(Customization)Í∞Ä Í∞ÄÎä•ÌïòÎã§Îäî Ï†êÏóê ÏûàÎã§. Ï¶â ÏÇ¨Ïö©ÏûêÍ∞Ä Ïù¥ÎØ∏ Îì±Î°ùÎêòÏñ¥ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Îì±Î°ùÎêú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Ïóê ÏùòÌï¥ ÌÉÄÍ≤üÏùÑ Ï†ïÌôïÌûà ÏÑ†Ï†ïÌï† Ïàò ÏûàÎäî Í≤ÉÏù¥Îã§.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3820/2475243331.py:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vllm import SamplingParams\n",
    "from uuid import uuid4\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "result = await retriever_node(qa_input)\n",
    "qa_input.retrieved_texts = result[\"retrieved_texts\"]\n",
    "qa_input.similarity_score = result[\"similarity_score\"]\n",
    "print(result)\n",
    "\n",
    "async def question_node(state: QAState) -> dict:\n",
    "\n",
    "    retrieved = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "\n",
    "    prompt = prompt1.format(\n",
    "        til=state.til,\n",
    "        level=state.level,\n",
    "        retrieved=retrieved\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[2], \n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"question\": final_text\n",
    "    }\n",
    "\n",
    "# q = await question_node(qa_input)\n",
    "# qa_input.question = q[\"question\"]\n",
    "# print(q[\"question\"])\n",
    "\n",
    "async def answer_node(state: QAState) -> dict:\n",
    "    if not state.question:\n",
    "        raise ValueError(\"ÏßàÎ¨∏Ïù¥ ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä generate_nodeÎ•º ÌÜµÌï¥ ÏßàÎ¨∏ÏùÑ ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§.\")\n",
    "\n",
    "    context = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    \n",
    "    prompt =f\"\"\"\n",
    "    ÎãπÏã†ÏùÄ ÏÇ¨Ïö©ÏûêÏùò Í∏∞Ïà† ÌïôÏäµ Í∏∞Î°ùÏùÑ Î∞îÌÉïÏúºÎ°ú, ÌïòÎÇòÏùò Í∏∞Ïà† Î©¥Ï†ë ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÎäî AIÏûÖÎãàÎã§.\n",
    "\n",
    "    ÏïÑÎûò Ï†ïÎ≥¥Î•º Ï∞∏Í≥†ÌïòÏó¨,\n",
    "    ÏßàÎ¨∏: {state.question}\n",
    "    ÏÇ¨Ïö©Ïûê TIL: {state.til}\n",
    "    level: {state.level}\n",
    "    - Ï∞∏Í≥† Î¨∏ÏÑú {context}\n",
    "\n",
    "    ‚Äª levelÏóê Îî∞Îùº ÏßàÎ¨∏ ÏàòÏ§ÄÏùÑ Ï°∞Ï†àÌï¥ÏÑú ÏßàÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥Ä \"\"1Í∞ú\"\"Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:\n",
    "    - level \"1\": ÍπäÏùÄ Í∏∞Ïà† Ïù¥Ìï¥ÏôÄ Ïã§Î¨¥ Í≤ΩÌóò Í∏∞Î∞ò ÏßàÎ¨∏\n",
    "    - level \"2\": Í∞úÎÖêÏ†Å Ïù¥Ìï¥Î•º Î¨ªÎäî ÏßàÎ¨∏\n",
    "    - level \"3\": Í∏∞Î≥∏ Í∞úÎÖêÏùÑ Î¨ªÎäî ÏßàÎ¨∏\n",
    "\n",
    "    Î∞òÎìúÏãú ÌïòÎÇòÏùò ÎãµÎ≥ÄÎßå ÎßåÎì§Í≥† **ÌïúÍµ≠Ïñ¥**Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.\n",
    "    ÏïÑÎûòÏôÄ Í∞ôÏùÄ ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•ÌïòÎêò, ÎãµÎ≥ÄÏóê ÏÉÅÍ¥ÄÏóÜÎäî Í∏∞Ìò∏ÎÇò Î¨∏ÏûêÎäî ÎπºÏ£ºÏÑ∏Ïöî.\n",
    "    ÎãµÎ≥Ä: \" \"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        stop_token_ids=[2],\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"answer\": final_text,\n",
    "        \"content\": [\n",
    "            ContentState(\n",
    "                question=state.question,\n",
    "                answer=final_text\n",
    "            )\n",
    "        ]        \n",
    "    }\n",
    "\n",
    "# a = await answer_node(qa_input)\n",
    "# qa_input.answer = a[\"answer\"]\n",
    "# qa_input.content = a[\"content\"]\n",
    "\n",
    "# print(\"üßæ ÏßàÎ¨∏:\", qa_input.content[0].question)\n",
    "# print(\"üí¨ ÎãµÎ≥Ä:\", qa_input.content[0].answer)\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    content_items = state.content or []\n",
    "\n",
    "    combined = \"\\n\".join(\n",
    "        f\"Q: {item.question}\\nA: {item.answer}\" for item in content_items\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Îã§ÏùåÏùÄ Î©¥Ï†ë ÏßàÎ¨∏Í≥º Í∑∏Ïóê ÎåÄÌïú ÎãµÎ≥ÄÏûÖÎãàÎã§. ÏßàÎ¨∏Í≥º ÎãµÎ≥ÄÏùò ÎÇ¥Ïö©ÏùÑ Î≥¥Í≥† ÌïµÏã¨ Ï£ºÏ†úÎ•º Ìïú Ï§ÑÎ°ú Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.  \n",
    "    Ïù¥ Ï†úÎ™©ÏùÄ Í∞úÎ∞ú Î¨∏ÏÑúÎÇò Í∏∞Îä• ÏÑ§Î™ÖÏÑúÏóêÏÑú Ïì∏ Ïàò ÏûàÏùÑ Ï†ïÎèÑÎ°ú Í∞ÑÍ≤∞ÌïòÍ≥† Íµ¨Ï≤¥Ï†ÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "    ÏòàÏãú:\n",
    "    Q: REST APIÎûÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?  \n",
    "    A: REST APIÎäî HTTP ÌîÑÎ°úÌÜ†ÏΩúÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏûêÏõêÏùÑ URIÎ°ú ÌëúÌòÑÌïòÍ≥†, CRUDÎ•º HTTP Î©îÏÑúÎìúÎ°ú ÏàòÌñâÌïòÎäî ÏïÑÌÇ§ÌÖçÏ≤òÏûÖÎãàÎã§.  \n",
    "    ‚Üí Ï†úÎ™©: REST API Í∞úÎÖê Î∞è Íµ¨ÏÑ± ÏöîÏÜå\n",
    "\n",
    "    Îã§Ïùå ÏßàÎ¨∏Í≥º ÎãµÎ≥ÄÏùÑ Ï∞∏Í≥†Ìï¥ÏÑú ÏúÑÏôÄ Í∞ôÏùÄ ÌòïÏãùÏúºÎ°ú **15Í∏ÄÏûê** Ïù¥ÎÇ¥Î°ú Ï†úÎ™©ÏùÑ Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî.\n",
    "\n",
    "    {combined}\n",
    "\n",
    "    ‚Üí Ï†úÎ™©:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.3,\n",
    "        max_tokens=32,\n",
    "        stop_token_ids=[2]\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(\n",
    "        prompt=prompt,\n",
    "        sampling_params=sampling_params,\n",
    "        request_id=request_id\n",
    "    ):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    return {\n",
    "        \"summary\": final_text \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43299b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 09:49:26 [async_llm.py:228] Added request 055832f9-c1b2-4b14-9a68-c35b26107530.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2433/2708729159.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 09:49:40 [async_llm.py:228] Added request c2bd692e-1409-43c3-898f-b9081ba77187.\n",
      "INFO 05-12 09:50:42 [async_llm.py:228] Added request d46e1a9a-c8dc-49e1-8d79-bce1fed8cbd3.\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# ÏãúÏûë ÏßÄÏ†ê ÏÑ§Ï†ï\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "\n",
    "# ÎÖ∏Îìú ÏÑ†Ïñ∏\n",
    "workflow.add_node(\"retriever\",retriever_node)\n",
    "\n",
    "workflow.add_node(\"question_generate\",question_node)\n",
    "workflow.add_node(\"answer_generate\",answer_node)\n",
    "\n",
    "workflow.add_node(\"summary_generate\",summary_node)\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question_generate\")\n",
    "workflow.add_edge(\"question_generate\", \"answer_generate\")\n",
    "workflow.add_edge(\"answer_generate\", \"summary_generate\")\n",
    "\n",
    "# Ï¢ÖÎ£å ÏßÄÏ†ê ÏÑ§Ï†ï\n",
    "workflow.set_finish_point(\"summary_generate\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c4dcfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 'ConconDev',\n",
       " 'date': '2024-09-06',\n",
       " 'level': 1,\n",
       " 'title': 'ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§ Í∞úÏÑ†: SseEmitter ÌôúÏö© Î∞è ÏÇ¨Ïö©ÏûêÎ≥Ñ Emitter Í¥ÄÎ¶¨',\n",
       " 'keywords': ['SSE', 'SseEmitter', 'ÏïåÎûå Íµ¨ÎèÖ', 'ÏÇ¨Ïö©ÏûêÎ≥Ñ Í¥ÄÎ¶¨'],\n",
       " 'til': '# ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§ Í∞úÏÑ†: SseEmitter ÌôúÏö© Î∞è ÏÇ¨Ïö©ÏûêÎ≥Ñ Emitter Í¥ÄÎ¶¨\\n\\n## 1. Ïò§Îäò Î∞∞Ïö¥ ÎÇ¥Ïö©\\n\\nÏò§Îäò Ï†ÄÎäî ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`Î•º ÌôúÏö©ÌïòÍ≥†, Í∞Å ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Ï†ÄÏû•ÌïòÍ≥† Í¥ÄÎ¶¨ÌïòÎäî Í∏∞Îä•ÏùÑ Íµ¨ÌòÑÌñàÏäµÎãàÎã§.  `SseEmitter`Îäî ÏÑúÎ≤ÑÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞Î•º Ïä§Ìä∏Î¶¨Î∞çÌïòÎäî Îç∞ Ïú†Ïö©Ìïú APIÏûÖÎãàÎã§.\\n\\n## 2. Í∞úÎÖê Ï†ïÎ¶¨\\n\\n*   **SSE (Server-Sent Events):** ÏÑúÎ≤ÑÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Îã®Î∞©Ìñ• ÌÜµÏã†ÏùÑ Í∞ÄÎä•ÌïòÍ≤å ÌïòÎäî Ïõπ Í∏∞Ïà†ÏûÖÎãàÎã§. ÏÑúÎ≤ÑÍ∞Ä ÏÉàÎ°úÏö¥ Ïù¥Î≤§Ìä∏ Î∞úÏÉù Ïãú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÍ≤å ÏûêÎèôÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Î•º Ï†ÑÏÜ°Ìï©ÎãàÎã§.\\n*   **SseEmitter:** SSE ÌÜµÏã†ÏùÑ ÏúÑÌïú Í∞ùÏ≤¥ÏûÖÎãàÎã§.  Îç∞Ïù¥ÌÑ∞Î•º Î∞úÌñâÌïòÍ±∞ÎÇò, Ïó∞Í≤∞ÏùÑ Ï¢ÖÎ£åÌïòÍ±∞ÎÇò, Ïò§Î•òÎ•º Ï≤òÎ¶¨ÌïòÎäî Îì±Ïùò Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\\n*   **ConcurrentHashMap:** Ïó¨Îü¨ Ïä§Î†àÎìúÏóêÏÑú ÎèôÏãúÏóê Ï†ëÍ∑ºÌï¥ÎèÑ ÏïàÏ†ÑÌïòÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§.  Ïó¨Í∏∞ÏÑúÎäî ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú ÌïòÍ≥† `SseEmitter`Î•º Í∞íÏúºÎ°ú Ï†ÄÏû•ÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÏóàÏäµÎãàÎã§.\\n\\n## 3. Ìï¥Îãπ Í∞úÎÖêÏù¥ ÌïÑÏöîÌïú Ïù¥Ïú†\\n\\nÍ∏∞Ï°¥ ÏïåÎûå Íµ¨ÎèÖ Î∞©ÏãùÏùÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÏùÑ Î≥¥ÎÇ¥Îäî Î∞©ÏãùÏúºÎ°ú, ÏÑúÎ≤Ñ Î∂ÄÌïòÍ∞Ä Ïã¨ÌïòÍ≥† Ìö®Ïú®ÏÑ±Ïù¥ Îñ®Ïñ¥Ï°åÏäµÎãàÎã§. `SseEmitter`Î•º ÏÇ¨Ïö©ÌïòÎ©¥ ÏÑúÎ≤ÑÎäî ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌñàÏùÑ ÎïåÎßå ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏÜ°ÌïòÎØÄÎ°ú, ÏÑúÎ≤Ñ ÏûêÏõêÏùÑ Ï†àÏïΩÌïòÍ≥† Ïã§ÏãúÍ∞ÑÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç®, ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.\\n\\n## 4. Í∞úÎÖêÏùÑ ÌôúÏö©ÌïòÎäî Î∞©Î≤ï\\n\\n1.  `SseEmitter` Í∞ùÏ≤¥Î•º ÏÉùÏÑ±ÌïòÍ≥† Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.\\n2.  ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú, `SseEmitter` Í∞ùÏ≤¥Î•º Í∞íÏúºÎ°ú `ConcurrentHashMap`Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§.\\n3.  ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÏÑú SSE Ïó∞Í≤∞ÏùÑ ÏÑ§Ï†ïÌïòÍ≥†, ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏã†Ìï©ÎãàÎã§.\\n4.  ÏÇ¨Ïö©ÏûêÍ∞Ä ÏïåÎûå Íµ¨ÎèÖ/Ï∑®ÏÜå ÏöîÏ≤≠ÏùÑ ÌïòÎ©¥, Ìï¥Îãπ ÏÇ¨Ïö©ÏûêÏùò `SseEmitter`Î•º ÏóÖÎç∞Ïù¥Ìä∏ÌïòÍ±∞ÎÇò ÏÇ≠Ï†úÌï©ÎãàÎã§.\\n\\n## 5. Î¨∏Ï†ú Ìï¥Í≤∞ Í≥ºÏ†ï\\n\\n*   Ï≤òÏùåÏóêÎäî `SseEmitter`Ïùò ÏÉùÎ™ÖÏ£ºÍ∏∞Î•º Ï†úÎåÄÎ°ú Í¥ÄÎ¶¨ÌïòÏßÄ Î™ªÌïòÏó¨ Î©îÎ™®Î¶¨ ÎàÑÏàòÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.  `SseEmitter` Í∞ùÏ≤¥Ïùò `close()` Î©îÏÑúÎìúÎ•º Ìò∏Ï∂úÌïòÏó¨ Î¶¨ÏÜåÏä§ ÎàÑÏàòÎ•º Î∞©ÏßÄÌñàÏäµÎãàÎã§.\\n*   ÏÇ¨Ïö©Ïûê ID Ï§ëÎ≥µ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú ÏÇ¨Ïö©ÌïòÎäî `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§.\\n*   ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÏÑú SSE Ïó∞Í≤∞ÏùÑ ÏïàÏ†ïÏ†ÅÏúºÎ°ú Ïú†ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ ÏóêÎü¨ Ìï∏Îì§ÎßÅ Î°úÏßÅÏùÑ Ï∂îÍ∞ÄÌñàÏäµÎãàÎã§.\\n\\n## 6. ÌïòÎ£® ÌöåÍ≥†\\n\\nÏò§ÎäòÏùÄ ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò ÏÑ±Îä• Í∞úÏÑ†ÏùÑ ÏúÑÌï¥ Ï§ëÏöîÌïú Í∏∞Ïà†Ïù∏ `SseEmitter`Î•º ÏùµÌûàÍ≥† Ï†ÅÏö©ÌïòÎäî ÏãúÍ∞ÑÏùÑ Í∞ÄÏ°åÏäµÎãàÎã§.  `SseEmitter`Ïùò ÎèôÏûë ÏõêÎ¶¨Î•º Ïù¥Ìï¥ÌïòÍ≥†, Ïã§Ï†ú ÏÑúÎπÑÏä§Ïóê Ï†ÅÏö©ÌïòÎ©¥ÏÑú ÎßéÏùÄ Ïñ¥Î†§ÏõÄÏùÑ Í≤™ÏóàÏßÄÎßå, Í≤∞Íµ≠ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Íµ¨ÌòÑÌï† Ïàò ÏûàÏóàÏäµÎãàÎã§. ÏïûÏúºÎ°úÎäî `SseEmitter`Î•º Îçî ÍπäÏù¥ Ïù¥Ìï¥ÌïòÍ≥†, Îã§ÏñëÌïú ÏùëÏö© Î∂ÑÏïºÏóê Ï†ÅÏö©Ìï¥Î≥¥Í≥† Ïã∂ÏäµÎãàÎã§.\\n\\n## 7. Ï†ÑÏ≤¥Ï†ÅÏúºÎ°ú Í∞úÏ°∞Ïãù Î¨∏Ïû• Íµ¨ÏÑ±\\n\\n*   **Î™©Ìëú:** ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò Ïã§ÏãúÍ∞ÑÏÑ± Î∞è Ìö®Ïú®ÏÑ± Ìñ•ÏÉÅ\\n*   **ÌïµÏã¨ Í∏∞Ïà†:** SSE, SseEmitter, ConcurrentHashMap\\n*   **Íµ¨ÌòÑ Îã®Í≥Ñ:**\\n    *   `SseEmitter` Í∞ùÏ≤¥ ÏÉùÏÑ± Î∞è Ï¥àÍ∏∞Ìôî\\n    *   ÏÇ¨Ïö©ÏûêÎ≥Ñ `SseEmitter` Ï†ÄÏû• Î∞è Í¥ÄÎ¶¨ (`ConcurrentHashMap`) \\n    *   SSE Ïó∞Í≤∞ ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ ÏàòÏã†\\n    *   ÏïåÎûå Íµ¨ÎèÖ/Ï∑®ÏÜå ÏöîÏ≤≠ Ï≤òÎ¶¨\\n    *   Î©îÎ™®Î¶¨ ÎàÑÏàò Î∞©ÏßÄ Î∞è ÏóêÎü¨ Ìï∏Îì§ÎßÅ\\n\\n',\n",
       " 'retrieved_texts': ['ÏÑúÎ≤Ñ Ï†ÑÏÜ° Ïù¥Î≤§Ìä∏(Server-sent events, SSE)Îäî ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä HTTP Ïó∞Í≤∞ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÏã†Ìï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî ÏÑúÎ≤Ñ Ìë∏Ïãú Í∏∞Ïà†Ïù¥Î©∞, Ï¥àÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞Ïù¥ ÏÑ§Ï†ïÎêú ÌõÑ ÏÑúÎ≤ÑÍ∞Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º Ìñ•Ìïú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏÜ°ÏùÑ ÏãúÏûëÌïòÎäî Î∞©Î≤ïÏùÑ ÏÑ§Î™ÖÌïúÎã§. Ïù¥Îäî ÏùºÎ∞òÏ†ÅÏúºÎ°ú Î∏åÎùºÏö∞Ï†Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Î©îÏãúÏßÄ ÏóÖÎç∞Ïù¥Ìä∏ ÎòêÎäî ÏßÄÏÜçÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ïä§Ìä∏Î¶ºÏùÑ Î≥¥ÎÇ¥Îäî Îç∞ ÏÇ¨Ïö©ÎêòÎ©∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ïù¥Î≤§Ìä∏ Ïä§Ìä∏Î¶ºÏùÑ ÏàòÏã†ÌïòÍ∏∞ ÏúÑÌï¥ ÌäπÏ†ï URLÏùÑ ÏöîÏ≤≠ÌïòÎäî EventSourceÎùºÎäî ÏûêÎ∞îÏä§ÌÅ¨Î¶ΩÌä∏ APIÎ•º ÌÜµÌï¥ Í∏∞Î≥∏ Î∏åÎùºÏö∞Ï†Ä Í∞Ñ Ïä§Ìä∏Î¶¨Î∞çÏùÑ Ìñ•ÏÉÅÏãúÌÇ§ÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÎã§. EventSource APIÎäî WHATWGÏóê ÏùòÌï¥ HTML Living StandardÏùò ÏùºÎ∂ÄÎ°ú ÌëúÏ§ÄÌôîÎêòÏóàÎã§. SSEÏùò ÎØ∏ÎîîÏñ¥ Ïú†ÌòïÏùÄ text/event-streamÏù¥Îã§. ÌååÏù¥Ïñ¥Ìè≠Ïä§ 6+, Íµ¨Í∏Ä ÌÅ¨Î°¨ 6+, Ïò§ÌéòÎùº 11.5+, ÏÇ¨ÌååÎ¶¨ 5+, ÎßàÏù¥ÌÅ¨Î°úÏÜåÌîÑÌä∏ Ïó£ÏßÄ 79+ Îì± Î™®Îì† ÏµúÏã† Î∏åÎùºÏö∞Ï†ÄÎäî ÏÑúÎ≤ÑÏóêÏÑú Ï†ÑÏÜ°ÎêòÎäî Ïù¥Î≤§Ìä∏Î•º ÏßÄÏõêÌïúÎã§.',\n",
       "  'Ìë∏Ïãú Í∏∞Î≤ï Í∏∞Ïà†ÏùÄ ÌíÄ Í∏∞Î≤ïÏúºÎ°ú Î™ÖÎ™ÖÎêú ÏõπÎ∏åÎùºÏö∞Ï†ÄÏôÄ ÎπÑÍµêÌï† Ïàò ÏûàÎã§. ÌòÑÏû¨ Ïù∏ÌÑ∞ÎÑ∑ÏóêÏÑú ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÎäî ÏõπÎ∏åÎùºÏö∞Ï†ÄÎäî ÏÇ¨Ïö©Ìï† ÎïåÏóê ÏÇ¨Ïö©ÏûêÍ∞Ä Ìï≠ÏÉÅ ÏûêÏã†Ïù¥ Í∞ñÍ≥†Ïûê ÌïòÎäî Ï†ïÎ≥¥Î•º ÏÜåÏú†Ìïú ÏÑúÎ≤ÑÏóêÍ≤å Ï†ïÎ≥¥Î•º ÏöîÏ≤≠ÌïòÎ©∞ ÏõπÎ∏åÎùºÏö∞Ï†ÄÏùò ÏÇ¨Ïö©Í≥º Î™©Ï†ÅÏßÄÏóê ÎåÄÌïú ÏµúÏ¢ÖÍ≤∞Ï†ï Ïó≠Ïãú ÏÇ¨Ïö©ÏûêÍ∞Ä Í≤∞Ï†ïÌï† Ïàò ÏûàÎã§. Î∞òÎ©¥ Ìë∏Ïãú Í∏∞Î≤ïÏùÄ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏùºÏùºÏù¥ ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏïÑÎèÑ ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏûêÎèôÏúºÎ°ú Îâ¥Ïä§ÎÇò ÏÇ¨Ïö©ÏûêÍ∞Ä ÏõêÌïòÎäî ÌäπÎ≥ÑÌïú Ï†ïÎ≥¥, ÏòàÎ•º Îì§Î©¥ Ï¶ùÍ∂åÏãúÏû•Ïùò Ï£ºÍ∏∞Ï†ÅÏù∏ Ï†ïÎ≥¥ Í∞ôÏùÄ Í≤ÉÏùÑ Ï†úÍ≥µÌïúÎã§. Ïù¥ Îëê Í∞ÄÏßÄ Í∏∞Î≤ïÏùò Ï∞®Ïù¥Ï†êÏùÄ Ï†ïÎ≥¥Ïùò ÌùêÎ¶ÑÏùÑ ÎàÑÍ∞Ä ÌÜµÏ†úÌïòÎäêÎÉê ÌïòÎäî Ï†êÏóê ÏûàÎã§Í≥† Ìï† Ïàò ÏûàÎã§. ÌíÄ Í∏∞Î≤ï ÌïòÏóêÏÑúÎäî ÏÇ¨Ïö©Ïûê (Ï¶â ÏÜåÎπÑÏûê)Îì§Ïù¥ Ï†ïÎ≥¥ Ï∑®Îìù Î∞è Ï†ïÎ≥¥Ïùò Ï†ëÏ¥âÏùÑ ÎßàÏùåÎåÄÎ°ú ÌÜµÏ†úÌï† Ïàò ÏûàÏúºÎÇò, Ìë∏Ïãú Í∏∞Î≤ï ÌïòÏóêÏÑúÎäî Ï†ïÎ≥¥Î•º Ï†ÑÎã¨ÌïòÎäî Ï™ΩÏóêÏÑú(Ï¶â Í¥ëÍ≥†Ï£º)Ï†ïÎ≥¥Ïùò ÌùêÎ¶ÑÏùÑ ÏßÅÏ†ë ÌÜµÏ†úÌï† Ïàò ÏûàÍ≤å ÎêúÎã§.',\n",
       "  'Ìë∏Ïãú Í∏∞Î≤ïÏùò Í∞ÄÏû• ÌÅ∞ Ïù¥Ï†êÏùÄ Ïó≠Ïãú Ï†ïÎ≥¥Ïùò ÎßûÏ∂§Ìôî(Customization)Í∞Ä Í∞ÄÎä•ÌïòÎã§Îäî Ï†êÏóê ÏûàÎã§. Ï¶â ÏÇ¨Ïö©ÏûêÍ∞Ä Ïù¥ÎØ∏ Îì±Î°ùÎêòÏñ¥ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Îì±Î°ùÎêú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Ïóê ÏùòÌï¥ ÌÉÄÍ≤üÏùÑ Ï†ïÌôïÌûà ÏÑ†Ï†ïÌï† Ïàò ÏûàÎäî Í≤ÉÏù¥Îã§.'],\n",
       " 'similarity_score': 0.52782375,\n",
       " 'question0': 'ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Ïñ¥Îñ§ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÍπå? Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏùÄ Î¨¥Ïä® ÏÉÅÌô©ÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Í≤ÉÏûÖÎãàÍπå?\\nÏ¶â, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SSEÏôÄ `SseEmitter`, `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Ïù¥Ïú†ÏôÄ ÏÇ¨Ïö© ÏÉÅÌô©Ïóê ÎåÄÌï¥ ÏßàÎ¨∏ÌïòÍ≥† ÏûàÏäµÎãàÎã§.',\n",
       " 'question1': 'ÏßàÎ¨∏: \"ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SseEmitterÏôÄ ConcurrentHashMapÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÎäîÎç∞, Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Ïñ¥Îñ§ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÍπå?\"\\n\\nÏù¥ Íµ¨ÏÑ±ÏóêÎäî ÏïåÎûå ÏÑúÎπÑÏä§Ïùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Í≥†, Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§. Ïù¥ Íµ¨ÏÑ±ÏùÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÏùÑ Î≥¥ÎÇ¥Îäî Í∏∞Ï°¥Ïùò ÏïåÎûå Íµ¨ÎèÖ Î∞©ÏãùÏóê ÎπÑÌï¥ ÏÑúÎ≤Ñ Î∂ÄÌïòÍ∞Ä Í∞êÏÜåÌïòÍ≥†, Ïã§ÏãúÍ∞ÑÏÑ±Ïù¥ ÎÜíÏïÑÏßÄÎäî Ïù¥Ï†êÏù¥ ÏûàÏäµÎãàÎã§. ÎòêÌïú ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú SseEmitterÎ•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç® ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.',\n",
       " 'question2': 'ÏßàÎ¨∏: \"Ìë∏Ïãú Í∏∞Î≤ïÏù¥ ÌíÄ Í∏∞Î≤ïÍ≥º Ïñ¥Îñ§ Ï∞®Ïù¥Ï†êÏù¥ ÏûàÏäµÎãàÍπå? Ìë∏Ïãú Í∏∞Î≤ïÏùò Ïû•Ï†êÏùÄ Î¨¥ÏóáÏûÖÎãàÍπå? Ïù¥Îü¨Ìïú Í∏∞Ïà†ÏùÄ Ïñ¥Îñ§ Î∂ÑÏïºÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÏäµÎãàÍπå?',\n",
       " 'content0': ContentState(question='ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Ïñ¥Îñ§ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÍπå? Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏùÄ Î¨¥Ïä® ÏÉÅÌô©ÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Í≤ÉÏûÖÎãàÍπå?\\nÏ¶â, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SSEÏôÄ `SseEmitter`, `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Ïù¥Ïú†ÏôÄ ÏÇ¨Ïö© ÏÉÅÌô©Ïóê ÎåÄÌï¥ ÏßàÎ¨∏ÌïòÍ≥† ÏûàÏäµÎãàÎã§.', answer='ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§ÏóêÏÑú Ïã§ÏãúÍ∞ÑÏÑ±Í≥º Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÎêòÎäî Í∏∞Ïà† Ï§ë ÌïòÎÇòÎäî SSE(Server-Sent Events)ÏûÖÎãàÎã§. SSEÎäî ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä HTTP Ïó∞Í≤∞ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÏã†ÌïòÎäî ÏÑúÎ≤Ñ Ìë∏Ïãú Í∏∞Ïà†ÏûÖÎãàÎã§. Ïù¥ Í∏∞Ïà†ÏùÑ ÌôúÏö©ÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌï† ÎïåÎßàÎã§ Ìï¥Îãπ ÏïåÎûåÏùÑ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Ï†ÑÏÜ°Ìï† Ïàò ÏûàÏäµÎãàÎã§. SSEÎäî ÏµúÏã† Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú ÏûêÎèôÏúºÎ°ú ÏßÄÏõêÌïòÎ©∞, Ïù¥Î•º ÏúÑÌï¥ EventSourceÎùºÎäî ÏûêÎ∞îÏä§ÌÅ¨Î¶ΩÌä∏ APIÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\\n\\n            Ïù¥Îü¨Ìïú Í∏∞Ïà†ÏùÑ Íµ¨ÌòÑÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. `SseEmitter`Îäî ÏÑúÎ≤ÑÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Îç∞Ïù¥ÌÑ∞Î•º Ïä§Ìä∏Î¶¨Î∞çÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÎ©∞, `ConcurrentHashMap`ÏùÄ Ïó¨Îü¨ Ïä§Î†àÎìúÏóêÏÑú ÎèôÏãúÏóê Ï†ëÍ∑ºÌï¥ÎèÑ ÏïàÏ†ÑÌïòÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§. ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú, `SseEmitter`Î•º Í∞íÏúºÎ°ú `ConcurrentHashMap`Ïóê Ï†ÄÏû•ÌïòÏó¨ Í∞Å ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÏÑúÎ≤ÑÎäî ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌïòÎ©¥ Ìï¥Îãπ ÏÇ¨Ïö©ÏûêÏùò `SseEmitter`Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ìï¥Îãπ ÏÇ¨Ïö©ÏûêÏóêÍ≤åÎßå ÏïåÎûåÏùÑ Ï†ÑÏÜ°Ìï† Ïàò ÏûàÏäµÎãàÎã§.\\n\\n            Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Î™á Í∞ÄÏßÄ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§. Ï≤òÏùåÏúºÎ°ú ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Îäî Ï†ïÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏïÑÎèÑ ÎêòÎØÄÎ°ú, ÏÑúÎ≤Ñ ÏûêÏõêÏùÑ Ï†àÏïΩÌï† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÑúÎ≤ÑÎäî ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌï† ÎïåÎßàÎã§ Ìï¥Îãπ ÏïåÎûåÏùÑ Ï†ÑÏÜ°ÌïòÎØÄÎ°ú, Ïã§ÏãúÍ∞ÑÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç® ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§. Îî∞ÎùºÏÑú, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏùÄ Îß§Ïö∞ Ïú†Ïö©Ìïú Ï†ëÍ∑º Î∞©ÏãùÏûÖÎãàÎã§.'),\n",
       " 'content1': ContentState(question='ÏßàÎ¨∏: \"ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SseEmitterÏôÄ ConcurrentHashMapÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÎäîÎç∞, Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Ïñ¥Îñ§ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÍπå?\"\\n\\nÏù¥ Íµ¨ÏÑ±ÏóêÎäî ÏïåÎûå ÏÑúÎπÑÏä§Ïùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Í≥†, Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§. Ïù¥ Íµ¨ÏÑ±ÏùÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÏùÑ Î≥¥ÎÇ¥Îäî Í∏∞Ï°¥Ïùò ÏïåÎûå Íµ¨ÎèÖ Î∞©ÏãùÏóê ÎπÑÌï¥ ÏÑúÎ≤Ñ Î∂ÄÌïòÍ∞Ä Í∞êÏÜåÌïòÍ≥†, Ïã§ÏãúÍ∞ÑÏÑ±Ïù¥ ÎÜíÏïÑÏßÄÎäî Ïù¥Ï†êÏù¥ ÏûàÏäµÎãàÎã§. ÎòêÌïú ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú SseEmitterÎ•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç® ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.', answer='Ïã§ÏãúÍ∞Ñ ÏïåÎ¶º ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SseEmitterÏôÄ ConcurrentHashMapÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. SseEmitterÎäî ÏÑúÎ≤ÑÍ∞Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏÜ°ÌïòÎäî Î∞©Î≤ïÏûÖÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÍ≥†, ÏÑúÎ≤Ñ Î∂ÄÌïòÎ•º Ï§ÑÏùº Ïàò ÏûàÏäµÎãàÎã§. ConcurrentHashMapÏùÄ Ïó¨Îü¨ Ïä§Î†àÎìúÏóêÏÑú ÎèôÏãúÏóê Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§. ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitterÎ•º Ï†ÄÏû•ÌïòÍ≥† Í¥ÄÎ¶¨ÌïòÏó¨ ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎ¶ºÎßå Ï†ÑÏÜ°Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎ¶º Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Íµ¨ÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§.'),\n",
       " 'content2': ContentState(question='ÏßàÎ¨∏: \"Ìë∏Ïãú Í∏∞Î≤ïÏù¥ ÌíÄ Í∏∞Î≤ïÍ≥º Ïñ¥Îñ§ Ï∞®Ïù¥Ï†êÏù¥ ÏûàÏäµÎãàÍπå? Ìë∏Ïãú Í∏∞Î≤ïÏùò Ïû•Ï†êÏùÄ Î¨¥ÏóáÏûÖÎãàÍπå? Ïù¥Îü¨Ìïú Í∏∞Ïà†ÏùÄ Ïñ¥Îñ§ Î∂ÑÏïºÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÏäµÎãàÍπå?', answer='Ìï¥Îãπ Í∏∞Ïà†ÏùÄ ÌíÄ Í∏∞Î≤ïÍ≥ºÏùò Ï∞®Ïù¥Ï†êÏúºÎ°ú Ïù∏ÌÑ∞ÎÑ∑ÏóêÏÑú ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÎäî ÎåÄÎ∂ÄÎ∂ÑÏùò Ïõπ Î∏åÎùºÏö∞Ï†ÄÏôÄ Îã§Î•¥Í≤å ÏÇ¨Ïö©ÏûêÍ∞Ä ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏïÑÎèÑ ÏûêÎèôÏúºÎ°ú ÏÇ¨Ïö©ÏûêÏóêÍ≤å Îâ¥Ïä§ÎÇò ÌäπÎ≥ÑÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÎäî Ìë∏Ïãú Í∏∞Î≤ïÏûÖÎãàÎã§. Ïù¥ Í∏∞Ïà†Ïùò Í∞ÄÏû• ÌÅ∞ Ïû•Ï†êÏùÄ Ï†ïÎ≥¥Ïùò ÎßûÏ∂§Ìôî, Ï¶â, Îì±Î°ùÎêú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÌÉÄÍ≤üÏùÑ Ï†ïÌôïÌûà ÏÑ†ÌÉùÌïòÏó¨ Ï†ïÎ≥¥Î•º Ï†ÑÎã¨Ìï† Ïàò ÏûàÎã§Îäî Ï†êÏûÖÎãàÎã§.'),\n",
       " 'content': [ContentState(question='ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Ïñ¥Îñ§ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÍπå? Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏùÄ Î¨¥Ïä® ÏÉÅÌô©ÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Í≤ÉÏûÖÎãàÍπå?\\nÏ¶â, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SSEÏôÄ `SseEmitter`, `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Ïù¥Ïú†ÏôÄ ÏÇ¨Ïö© ÏÉÅÌô©Ïóê ÎåÄÌï¥ ÏßàÎ¨∏ÌïòÍ≥† ÏûàÏäµÎãàÎã§.', answer='ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§ÏóêÏÑú Ïã§ÏãúÍ∞ÑÏÑ±Í≥º Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÎêòÎäî Í∏∞Ïà† Ï§ë ÌïòÎÇòÎäî SSE(Server-Sent Events)ÏûÖÎãàÎã§. SSEÎäî ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä HTTP Ïó∞Í≤∞ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏Î•º ÏàòÏã†ÌïòÎäî ÏÑúÎ≤Ñ Ìë∏Ïãú Í∏∞Ïà†ÏûÖÎãàÎã§. Ïù¥ Í∏∞Ïà†ÏùÑ ÌôúÏö©ÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌï† ÎïåÎßàÎã§ Ìï¥Îãπ ÏïåÎûåÏùÑ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Ï†ÑÏÜ°Ìï† Ïàò ÏûàÏäµÎãàÎã§. SSEÎäî ÏµúÏã† Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú ÏûêÎèôÏúºÎ°ú ÏßÄÏõêÌïòÎ©∞, Ïù¥Î•º ÏúÑÌï¥ EventSourceÎùºÎäî ÏûêÎ∞îÏä§ÌÅ¨Î¶ΩÌä∏ APIÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\\n\\n            Ïù¥Îü¨Ìïú Í∏∞Ïà†ÏùÑ Íµ¨ÌòÑÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. `SseEmitter`Îäî ÏÑúÎ≤ÑÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Îç∞Ïù¥ÌÑ∞Î•º Ïä§Ìä∏Î¶¨Î∞çÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÎ©∞, `ConcurrentHashMap`ÏùÄ Ïó¨Îü¨ Ïä§Î†àÎìúÏóêÏÑú ÎèôÏãúÏóê Ï†ëÍ∑ºÌï¥ÎèÑ ÏïàÏ†ÑÌïòÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§. ÏÇ¨Ïö©Ïûê IDÎ•º ÌÇ§Î°ú, `SseEmitter`Î•º Í∞íÏúºÎ°ú `ConcurrentHashMap`Ïóê Ï†ÄÏû•ÌïòÏó¨ Í∞Å ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÏÑúÎ≤ÑÎäî ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌïòÎ©¥ Ìï¥Îãπ ÏÇ¨Ïö©ÏûêÏùò `SseEmitter`Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ìï¥Îãπ ÏÇ¨Ïö©ÏûêÏóêÍ≤åÎßå ÏïåÎûåÏùÑ Ï†ÑÏÜ°Ìï† Ïàò ÏûàÏäµÎãàÎã§.\\n\\n            Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Î™á Í∞ÄÏßÄ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§. Ï≤òÏùåÏúºÎ°ú ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Ïùò Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Îäî Ï†ïÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏïÑÎèÑ ÎêòÎØÄÎ°ú, ÏÑúÎ≤Ñ ÏûêÏõêÏùÑ Ï†àÏïΩÌï† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÑúÎ≤ÑÎäî ÏÉàÎ°úÏö¥ ÏïåÎûåÏù¥ Î∞úÏÉùÌï† ÎïåÎßàÎã§ Ìï¥Îãπ ÏïåÎûåÏùÑ Ï†ÑÏÜ°ÌïòÎØÄÎ°ú, Ïã§ÏãúÍ∞ÑÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú, ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú `SseEmitter`Î•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç® ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§. Îî∞ÎùºÏÑú, ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ `SseEmitter`ÏôÄ `ConcurrentHashMap`ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏùÄ Îß§Ïö∞ Ïú†Ïö©Ìïú Ï†ëÍ∑º Î∞©ÏãùÏûÖÎãàÎã§.'),\n",
       "  ContentState(question='ÏßàÎ¨∏: \"ÏïåÎûå Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SseEmitterÏôÄ ConcurrentHashMapÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÎäîÎç∞, Ïù¥Îü¨Ìïú Íµ¨ÏÑ±ÏóêÎäî Ïñ¥Îñ§ Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÍπå?\"\\n\\nÏù¥ Íµ¨ÏÑ±ÏóêÎäî ÏïåÎûå ÏÑúÎπÑÏä§Ïùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Í≥†, Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÎäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§. Ïù¥ Íµ¨ÏÑ±ÏùÄ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÏÑúÎ≤ÑÏóê ÏöîÏ≤≠ÏùÑ Î≥¥ÎÇ¥Îäî Í∏∞Ï°¥Ïùò ÏïåÎûå Íµ¨ÎèÖ Î∞©ÏãùÏóê ÎπÑÌï¥ ÏÑúÎ≤Ñ Î∂ÄÌïòÍ∞Ä Í∞êÏÜåÌïòÍ≥†, Ïã§ÏãúÍ∞ÑÏÑ±Ïù¥ ÎÜíÏïÑÏßÄÎäî Ïù¥Ï†êÏù¥ ÏûàÏäµÎãàÎã§. ÎòêÌïú ÏÇ¨Ïö©ÏûêÎ≥ÑÎ°ú SseEmitterÎ•º Í¥ÄÎ¶¨Ìï®ÏúºÎ°úÏç® ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎûåÎßå Ï†ÑÏÜ°ÌïòÎèÑÎ°ù Ìï† Ïàò ÏûàÏñ¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎûå Íµ¨ÎèÖ ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.', answer='Ïã§ÏãúÍ∞Ñ ÏïåÎ¶º ÏÑúÎπÑÏä§Î•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ SseEmitterÏôÄ ConcurrentHashMapÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. SseEmitterÎäî ÏÑúÎ≤ÑÍ∞Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏóêÍ≤å Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏÜ°ÌïòÎäî Î∞©Î≤ïÏûÖÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÍ≥†, ÏÑúÎ≤Ñ Î∂ÄÌïòÎ•º Ï§ÑÏùº Ïàò ÏûàÏäµÎãàÎã§. ConcurrentHashMapÏùÄ Ïó¨Îü¨ Ïä§Î†àÎìúÏóêÏÑú ÎèôÏãúÏóê Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Í≤ÄÏÉâÌï† Ïàò ÏûàÎäî Ìï¥ÏãúÎßµÏûÖÎãàÎã§. ÏÇ¨Ïö©ÏûêÎ≥Ñ SseEmitterÎ•º Ï†ÄÏû•ÌïòÍ≥† Í¥ÄÎ¶¨ÌïòÏó¨ ÌäπÏ†ï ÏÇ¨Ïö©ÏûêÏóê ÎåÄÌïú ÏïåÎ¶ºÎßå Ï†ÑÏÜ°Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ ÎçîÏö± Ìö®Ïú®Ï†ÅÏù∏ ÏïåÎ¶º Íµ¨ÎèÖ ÏÑúÎπÑÏä§Î•º Íµ¨ÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§.'),\n",
       "  ContentState(question='ÏßàÎ¨∏: \"Ìë∏Ïãú Í∏∞Î≤ïÏù¥ ÌíÄ Í∏∞Î≤ïÍ≥º Ïñ¥Îñ§ Ï∞®Ïù¥Ï†êÏù¥ ÏûàÏäµÎãàÍπå? Ìë∏Ïãú Í∏∞Î≤ïÏùò Ïû•Ï†êÏùÄ Î¨¥ÏóáÏûÖÎãàÍπå? Ïù¥Îü¨Ìïú Í∏∞Ïà†ÏùÄ Ïñ¥Îñ§ Î∂ÑÏïºÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÏäµÎãàÍπå?', answer='Ìï¥Îãπ Í∏∞Ïà†ÏùÄ ÌíÄ Í∏∞Î≤ïÍ≥ºÏùò Ï∞®Ïù¥Ï†êÏúºÎ°ú Ïù∏ÌÑ∞ÎÑ∑ÏóêÏÑú ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÎäî ÎåÄÎ∂ÄÎ∂ÑÏùò Ïõπ Î∏åÎùºÏö∞Ï†ÄÏôÄ Îã§Î•¥Í≤å ÏÇ¨Ïö©ÏûêÍ∞Ä ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏïÑÎèÑ ÏûêÎèôÏúºÎ°ú ÏÇ¨Ïö©ÏûêÏóêÍ≤å Îâ¥Ïä§ÎÇò ÌäπÎ≥ÑÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÎäî Ìë∏Ïãú Í∏∞Î≤ïÏûÖÎãàÎã§. Ïù¥ Í∏∞Ïà†Ïùò Í∞ÄÏû• ÌÅ∞ Ïû•Ï†êÏùÄ Ï†ïÎ≥¥Ïùò ÎßûÏ∂§Ìôî, Ï¶â, Îì±Î°ùÎêú ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÌÉÄÍ≤üÏùÑ Ï†ïÌôïÌûà ÏÑ†ÌÉùÌïòÏó¨ Ï†ïÎ≥¥Î•º Ï†ÑÎã¨Ìï† Ïàò ÏûàÎã§Îäî Ï†êÏûÖÎãàÎã§.')],\n",
       " 'summary': 'Ìë∏Ïãú Í∏∞Ïà† vs ÌíÄ Í∏∞Ïà†: ÏûêÎèô Ï†ïÎ≥¥ Ï†úÍ≥µ,'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e569f1",
   "metadata": {},
   "source": [
    "#### Î†àÍ±∞Ïãú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f43b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from uuid import uuid4\n",
    "from vllm import SamplingParams\n",
    "import os\n",
    "\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    host=os.getenv(\"QDRANT_HOST\"), \n",
    "    port=os.getenv(\"QDRANT_PORT\"))\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cpu\")\n",
    "\n",
    "def embed_text(text: str) -> list[float]:\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "async def retriever_node(state: QAState) -> dict:\n",
    "    query = state.title + \" \" + \" \".join(state.keywords)\n",
    "    query_vector = embed_text(query)\n",
    "\n",
    "    collection_names = [\"ai\", \"cloud\", \"frontend\", \"backend\"]\n",
    "    best_score = 0.0\n",
    "    retrieved_texts: List[str] = []\n",
    "\n",
    "    for col in collection_names:\n",
    "        results = qdrant.search(\n",
    "            collection_name=col,\n",
    "            query_vector=query_vector,\n",
    "            limit=3,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        if results and results[0].score > best_score:\n",
    "            best_score = results[0].score\n",
    "            retrieved_texts = [h.payload[\"text\"] for h in results if \"text\" in h.payload]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"similarity_score\": best_score,\n",
    "        \"retrieved_texts\": retrieved_texts\n",
    "    }\n",
    "\n",
    "prom = \"\"\"\n",
    "ÎãπÏã†ÏùÄ ÏÇ¨Ïö©ÏûêÏùò Í∏∞Ïà† ÌïôÏäµ Í∏∞Î°ùÏùÑ Î∞îÌÉïÏúºÎ°ú, Í∏∞Ïà† Î©¥Ï†ë ÏßàÎ¨∏ÏùÑ ÏÉùÏÑ±ÌïòÎäî AIÏûÖÎãàÎã§.\n",
    "\n",
    "ÏïÑÎûò Ï†ïÎ≥¥Î•º Ï∞∏Í≥†ÌïòÏó¨,\n",
    "[TIL Î≥∏Î¨∏] {til}\n",
    "[RAG Í≤ÄÏÉâ Í≤∞Í≥º] {text}\n",
    "[ÏÑ†ÌÉùÌïú ÎÇúÏù¥ÎèÑ] {level}\n",
    "\n",
    "‚Äª levelÏóê Îî∞Îùº ÏßàÎ¨∏ ÏàòÏ§ÄÏùÑ Ï°∞Ï†àÌï¥ÏÑú Î©¥Ï†ë ÏßàÎ¨∏ÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:\n",
    "- level \"1\": ÍπäÏùÄ Í∏∞Ïà† Ïù¥Ìï¥ÏôÄ Ïã§Î¨¥ Í≤ΩÌóò Í∏∞Î∞ò ÏßàÎ¨∏\n",
    "- level \"2\": Í∞úÎÖêÏ†Å Ïù¥Ìï¥Î•º Î¨ªÎäî ÏßàÎ¨∏\n",
    "- level \"3\": Í∏∞Î≥∏ Í∞úÎÖêÏùÑ Î¨ªÎäî ÏßàÎ¨∏\n",
    "\n",
    "Î™®Îì† ÏßàÎ¨∏Í≥º ÎãµÎ≥ÄÏùÄ Î∞òÎìúÏãú **ÌïúÍµ≠Ïñ¥**Î°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.\n",
    "\n",
    "\"\"\"\n",
    "# ÏßàÎ¨∏ ÏÉùÏÑ± ÎÖ∏Îìú\n",
    "async def question0_node(state: QAState) -> Dict[str, Any]:\n",
    "    text = \"\\n\\n\".join(state.retrieved_texts or [])\n",
    "    prompt = prom.format(\n",
    "        til=state.til,\n",
    "        text=text,\n",
    "        level=state.level\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    request_id = str(uuid4())\n",
    "    final_text = \"\"\n",
    "\n",
    "    async for output in llm.generate(prompt, sampling_params, request_id=request_id):\n",
    "        final_text = output.outputs[0].text.strip()\n",
    "\n",
    "    # ÏßàÎ¨∏Îßå Ï∂îÏ∂ú\n",
    "    match = re.search(r\"ÏßàÎ¨∏[:Ôºö]\\s*(.+)\", final_text)\n",
    "    question = match.group(1).strip() if match else \"ÏßàÎ¨∏ ÏÉùÏÑ± Ïã§Ìå®\"\n",
    "\n",
    "    return {\n",
    "        \"qa_0\": ContentState(question=question, answer=\"\")\n",
    "    }\n",
    "\n",
    "async def question1_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q1: '{state.title}' Í¥ÄÎ†® Îëê Î≤àÏß∏ ÏßàÎ¨∏ÏûÖÎãàÎã§.\"\n",
    "    return {\"qa_1\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "async def question2_node(state: QAState) -> Dict[str, Any]:\n",
    "    generated_q = f\"Q2: '{state.title}' Í¥ÄÎ†® ÏÑ∏ Î≤àÏß∏ ÏßàÎ¨∏ÏûÖÎãàÎã§.\"\n",
    "    return {\"qa_2\": ContentState(question=generated_q, answer=\"\")}\n",
    "\n",
    "# ÎãµÎ≥Ä ÏÉùÏÑ± ÎÖ∏Îìú\n",
    "async def answer0_node(state: QAState) -> dict:\n",
    "    return {\"qa_0\": {\"question\": state.qa_0.question, \"answer\": \"ÎãµÎ≥Ä0\"}}\n",
    "\n",
    "\n",
    "async def summary_node(state: QAState) -> dict:\n",
    "    # state.qa_0, qa_1, qa_2Ïóê Îã¥Í∏¥ ÏßàÎ¨∏/ÎãµÎ≥ÄÏùÑ Î™®ÏïÑÏÑú ÏµúÏ¢Ö summary, content ÏÉùÏÑ±\n",
    "    content = [state.qa_0]\n",
    "    summary = \"ÏöîÏïΩ ÏÉùÏÑ± ÏôÑÎ£å\"\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3570c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/IPython/core/formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langgraph/pregel/__init__.py:634\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    633\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    635\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    287\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    288\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    289\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    290\u001b[39m         )\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    301\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai2-server/deeplearning/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:462\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    457\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m             msg = (\n\u001b[32m    459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    465\u001b[39m msg = (\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x7ff53e3d71d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7327878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f17a5eb6450>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Í∑∏ÎûòÌîÑ Ï†ïÏùò Î∞è Íµ¨ÏÑ±\n",
    "from langgraph.graph import END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "workflow = StateGraph(QAState)\n",
    "\n",
    "# ÎÖ∏Îìú ÏÑ†Ïñ∏\n",
    "workflow.add_node(\"retriever\", RunnableLambda(retriever_node).with_config({\"run_name\": \"retriever\"}))\n",
    "\n",
    "# ÏßàÎ¨∏ ÏÉùÏÑ± ÎÖ∏Îìú\n",
    "workflow.add_node(\"question0\", RunnableLambda(question0_node).with_config({\"run_name\": \"question0\"}))\n",
    "workflow.add_node(\"question1\", RunnableLambda(question1_node).with_config({\"run_name\": \"question1\"}))\n",
    "workflow.add_node(\"question2\", RunnableLambda(question2_node).with_config({\"run_name\": \"question2\"}))\n",
    "\n",
    "# ÎãµÎ≥Ä ÏÉùÏÑ± ÎÖ∏Îìú\n",
    "workflow.add_node(\"answer0\", RunnableLambda(answer0_node).with_config({\"run_name\": \"answer0\"}))\n",
    "workflow.add_node(\"answer1\", RunnableLambda(answer1_node).with_config({\"run_name\": \"answer1\"}))\n",
    "workflow.add_node(\"answer2\", RunnableLambda(answer2_node).with_config({\"run_name\": \"answer2\"}))\n",
    "\n",
    "# fallback + summary\n",
    "# workflow.add_node(\"fallback_generate\", RunnableLambda(fallback_generate_node).with_config({\"run_name\": \"fallback_generate\"}))\n",
    "workflow.add_node(\"summary_node\", RunnableLambda(summary_node).with_config({\"run_name\": \"summary_node\"}))\n",
    "\n",
    "# Ï°∞Í±¥Î∂Ä Î∂ÑÍ∏∞ ÏÑ§Ï†ï\n",
    "# def route_by_similarity(state: QAState) -> str:\n",
    "#     return \"generate_seq\" if state.similarity_score >= 0.5 else \"fallback_generate\"\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"retriever\",\n",
    "#     route_by_similarity,\n",
    "#     {\n",
    "#         \"generate_seq\": [\"question0\", \"question1\", \"question2\"],\n",
    "#         \"fallback_generate\": \"fallback_generate\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "workflow.add_edge(\"retriever\", \"question0\")\n",
    "workflow.add_edge(\"retriever\", \"question1\")\n",
    "workflow.add_edge(\"retriever\", \"question2\")\n",
    "\n",
    "\n",
    "# Í∞Å ÏßàÎ¨∏ ÏÉùÏÑ± ‚Üí ÎãµÎ≥Ä ÏÉùÏÑ± Ïó∞Í≤∞\n",
    "workflow.add_edge(\"question0\", \"answer0\")\n",
    "workflow.add_edge(\"question1\", \"answer1\")\n",
    "workflow.add_edge(\"question2\", \"answer2\")\n",
    "\n",
    "# Í∞Å ÎãµÎ≥Ä ÎÖ∏Îìú ‚Üí summary\n",
    "workflow.add_edge(\"answer0\", \"summary_node\")\n",
    "workflow.add_edge(\"answer1\", \"summary_node\")\n",
    "workflow.add_edge(\"answer2\", \"summary_node\")\n",
    "\n",
    "# fallbackÎèÑ summaryÎ°ú\n",
    "# workflow.add_edge(\"fallback_generate\", \"summary_node\")\n",
    "\n",
    "# ÏãúÏûë, Ï¢ÖÎ£å ÏßÄÏ†ê ÏÑ§Ï†ï\n",
    "workflow.set_entry_point(\"retriever\")\n",
    "workflow.set_finish_point(\"summary_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc49f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "# result = await graph.ainvoke(qa_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ïù¥Í±¥ ÏûÖÎ†•Ïóê ÎåÄÌïú ÏßàÎ¨∏ÏûÖÎãàÎã§: LangGraph Ïó∞Îèô ÌÖåÏä§Ìä∏'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from langsmith import traceable\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# @traceable(name=\"generate-question\")\n",
    "# def generate_question(input_text: str) -> str:\n",
    "#     return f\"Ïù¥Í±¥ ÏûÖÎ†•Ïóê ÎåÄÌïú ÏßàÎ¨∏ÏûÖÎãàÎã§: {input_text}\"\n",
    "\n",
    "# generate_question(\"LangGraph Ïó∞Îèô ÌÖåÏä§Ìä∏\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
